{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bbbb75-8c65-440e-a059-c63c2fa91996",
   "metadata": {},
   "source": [
    "purpose of this notebook is to finetune the \"distilbert/distilbert-base-uncased\" model\n",
    "Handles city, state and city-state separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7436463-26a4-4ebb-abd1-771ee134220b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6740344d-7d09-457a-bb68-a64f2b532103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6093b56-0180-4b67-8b04-b562979979ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples_v2.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples_v3.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v2.parquet\")\n",
    "full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v3.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f06a123-bdd7-4655-ae16-92bdc655924f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'id'],\n",
       "    num_rows: 300000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5b0e6e-e169-4e32-aeae-00c58949ff32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set_size = 5000\n",
    "val_start = len(full_dataset) - val_set_size\n",
    "val_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3d3524-3a60-4ce2-863b-08a6dadd2fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_dataset = full_dataset.select(range(val_start))  # Select training rows\n",
    "val_dataset = full_dataset.select(range(val_start, len(full_dataset)))  # Select last 1000 rows for validation\n",
    "\n",
    "# Combine them into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ffc3cd-886d-4f7f-a8c8-8f5413628646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id'],\n",
       "        num_rows: 295000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340175ec-d224-4c3b-aaa5-16037c61fff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'].to_pandas()['tokens'].apply(len).hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2046667-d9ef-44ab-a444-dc95752478aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f20d0f6-fa8e-47d7-a60b-2e673e25685a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 3688.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for distilbert-based NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Function to tokenize the input and align labels with tokens\n",
    "def tokenize_and_align_labels(example):\n",
    "    # Tokenize 'tokens' while keeping track of word boundaries\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example['tokens'], \n",
    "        is_split_into_words=True, \n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        max_length=64,\n",
    "    )\n",
    "    \n",
    "    # Get the word_ids (mapping from tokens to original words)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    aligned_labels = []\n",
    "\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)  # Special tokens ([CLS], [SEP], etc.)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(example['ner_tags'][word_idx])  # Assign the label to the first token of each word\n",
    "        else:\n",
    "            aligned_labels.append(-100)  # Subword tokens get label -100\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the function to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106935d0-9093-4ff9-8e0f-afe4aa2d792c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 295000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331d6dca-55d0-473e-94ca-90a152a9e9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['gatherin', 'in', 'Mount', 'Prospect'],\n",
       " 'ner_tags': [0, 0, 5, 6],\n",
       " 'id': 295000,\n",
       " 'input_ids': [101,\n",
       "  8587,\n",
       "  2378,\n",
       "  1999,\n",
       "  4057,\n",
       "  9824,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1221fd4e-3f16-4ed4-9d9f-c359604545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions_and_labels(predictions, references):\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    cmp_count = 0\n",
    "\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        # Only keep labels that are not -100\n",
    "        true_labels_example = [label for label in reference if label != -100]\n",
    "        \n",
    "        # Align predictions: Remove predictions for which the corresponding reference label is -100\n",
    "        true_predictions_example = [pred for pred, ref in zip(prediction, reference) if ref != -100]\n",
    "\n",
    "        # Ensure the length of predictions and labels matches\n",
    "        if len(true_predictions_example) == len(true_labels_example):\n",
    "            true_labels.append(true_labels_example)\n",
    "            true_predictions.append(true_predictions_example)\n",
    "            cmp_count += 1\n",
    "        else:\n",
    "            # Log or handle the error (example-level mismatch)\n",
    "            # print(f\"Skipping example due to mismatch: predictions ({len(true_predictions_example)}), labels ({len(true_labels_example)})\")\n",
    "            continue  # Skip this example\n",
    "\n",
    "    # Flatten the lists (convert from list of lists to a single list)\n",
    "    true_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
    "    true_labels = [label for sublist in true_labels for label in sublist]\n",
    "    print(f\"cmp_count = {cmp_count} out of {len(predictions)}\")\n",
    "\n",
    "    return true_predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f62dee2-077d-451f-af48-60eaf50e5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Post-process the predictions and labels to remove -100 values\n",
    "    true_predictions, true_labels = postprocess_predictions_and_labels(predictions, labels)\n",
    "\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7acd16-763a-4055-b492-3007b5057da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the NER label mappings\n",
    "# id2label = {\n",
    "#     0: \"O\",         # Outside any entity\n",
    "#     1: \"B-PER\",     # Beginning of a person entity\n",
    "#     2: \"I-PER\",     # Inside a person entity\n",
    "#     3: \"B-ORG\",     # Beginning of an organization entity\n",
    "#     4: \"I-ORG\",     # Inside an organization entity\n",
    "#     5: \"B-LOC\",     # Beginning of a location entity\n",
    "#     6: \"I-LOC\",     # Inside a location entity\n",
    "#     7: \"B-MISC\",    # Beginning of a miscellaneous entity\n",
    "#     8: \"I-MISC\"     # Inside a miscellaneous entity\n",
    "# }\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",        # Outside any named entity\n",
    "    1: \"B-PER\",    # Beginning of a person entity\n",
    "    2: \"I-PER\",    # Inside a person entity\n",
    "    3: \"B-ORG\",    # Beginning of an organization entity\n",
    "    4: \"I-ORG\",    # Inside an organization entity\n",
    "    5: \"B-CITY\",    # Beginning of a city entity\n",
    "    6: \"I-CITY\",    # Inside a city entity\n",
    "    7: \"B-STATE\",    # Beginning of a state entity\n",
    "    8: \"I-STATE\",    # Inside a state entity\n",
    "    9: \"B-CITYSTATE\",   # Beginning of a city_state entity\n",
    "   10: \"I-CITYSTATE\",   # Inside a city_state entity\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", \n",
    "                                                        num_labels=11, \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,  # Task type is token classification (NER)\n",
    "    r=8,  # Low-rank dimension (you can experiment with this)\n",
    "    lora_alpha=32,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    target_modules=['q_lin']  # LoRA is applied to query layer\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "343c2425-f8a2-4d13-a1cc-22ed6a4717a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c185799-1fd7-4319-a2a5-89ba1ff52df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110628' max='110628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110628/110628 1:48:43, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.007856</td>\n",
       "      <td>0.072262</td>\n",
       "      <td>0.308496</td>\n",
       "      <td>0.072262</td>\n",
       "      <td>0.082606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.064123</td>\n",
       "      <td>0.267346</td>\n",
       "      <td>0.064123</td>\n",
       "      <td>0.064793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.264350</td>\n",
       "      <td>0.063477</td>\n",
       "      <td>0.063507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.063262</td>\n",
       "      <td>0.268630</td>\n",
       "      <td>0.063262</td>\n",
       "      <td>0.062750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>0.266801</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>0.061850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.062487</td>\n",
       "      <td>0.264261</td>\n",
       "      <td>0.062487</td>\n",
       "      <td>0.061441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 3948 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 3948 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 3948 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 3948 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 3948 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 3948 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=110628, training_loss=0.03268636545453882, metrics={'train_runtime': 6526.7588, 'train_samples_per_second': 271.191, 'train_steps_per_second': 16.95, 'total_flos': 2.896756330752e+16, 'train_loss': 0.03268636545453882, 'epoch': 6.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Output directory\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate at the end of every epoch\n",
    "    learning_rate=2e-5,               # Learning rate\n",
    "    per_device_train_batch_size=16,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
    "    num_train_epochs=6,               # Number of training epochs\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    logging_dir='./logs',             # Directory for logging\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,                  # LoRA-wrapped model\n",
    "    args=training_args,                # Training arguments\n",
    "    train_dataset=tokenized_dataset['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],  # Validation dataset (if available)\n",
    "    tokenizer=tokenizer,               # Tokenizer\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82880e92-4d83-442a-9a1c-3a2386b1c942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] O\n",
      "new B-CITY\n",
      "york I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: New York\n",
      "Predicted entities: new york\n",
      "\n",
      "[CLS] O\n",
      "los B-CITY\n",
      "angeles I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Los Angeles\n",
      "Predicted entities: los angeles\n",
      "\n",
      "[CLS] O\n",
      "chicago B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Chicago\n",
      "Predicted entities: chicago\n",
      "\n",
      "[CLS] O\n",
      "philadelphia B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Philadelphia\n",
      "Predicted entities: philadelphia\n",
      "\n",
      "[CLS] O\n",
      "dallas B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Dallas\n",
      "Predicted entities: dallas\n",
      "\n",
      "[CLS] O\n",
      "fort B-CITY\n",
      "worth I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Fort Worth\n",
      "Predicted entities: fort worth\n",
      "\n",
      "[CLS] O\n",
      "houston B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Houston\n",
      "Predicted entities: houston\n",
      "\n",
      "[CLS] O\n",
      "atlanta B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Atlanta\n",
      "Predicted entities: atlanta\n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "manchester B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Manchester\n",
      "Predicted entities: manchester\n",
      "\n",
      "[CLS] O\n",
      "washington B-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "d I-CITYSTATE\n",
      ". I-CITYSTATE\n",
      "c I-CITYSTATE\n",
      ". I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Washington, D.C.\n",
      "Predicted entities: washington , d. c .\n",
      "\n",
      "[CLS] O\n",
      "ha B-CITY\n",
      "##gers B-CITY\n",
      "##town I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Hagerstown\n",
      "Predicted entities: hagerstown\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "francisco I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: San Francisco\n",
      "Predicted entities: san francisco\n",
      "\n",
      "[CLS] O\n",
      "oakland B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Oakland\n",
      "Predicted entities: oakland\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: San Jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: san jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: weather in san jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "boston B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: weather in Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "boston B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Weather in Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "boston B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: weather Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "boston B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Weather Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "weather O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Boston weather\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "weather O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: Boston Weather\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "i O\n",
      "love O\n",
      "pizza O\n",
      "##hu B-CITY\n",
      "##t O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: I love Pizzahut\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "i O\n",
      "like O\n",
      "starbucks O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: I like Starbucks\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "restaurants O\n",
      "in O\n",
      "sunny B-CITYSTATE\n",
      "##vale I-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "ca I-CITYSTATE\n",
      "[SEP] I-CITYSTATE\n",
      "Input: sushi restaurants in Sunnyvale, CA\n",
      "Predicted entities: sunnyvale , ca\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "restaurants O\n",
      "in O\n",
      "sunny B-CITYSTATE\n",
      "##vale I-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "california I-CITYSTATE\n",
      "[SEP] I-CITYSTATE\n",
      "Input: sushi restaurants in Sunnyvale, California\n",
      "Predicted entities: sunnyvale , california\n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "in O\n",
      "sf B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: ramen in sf\n",
      "Predicted entities: sf\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: sushi sf\n",
      "Predicted entities: sf\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITY\n",
      "##o O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: sushi sfo\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITYSTATE\n",
      "##o B-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "ca I-CITYSTATE\n",
      "[SEP] I-CITYSTATE\n",
      "Input: sushi sfo, CA\n",
      "Predicted entities: sfo , ca\n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "sf B-CITY\n",
      "##o O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: ramen sfo\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "sf B-CITY\n",
      "##o O\n",
      "su O\n",
      "##shi O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: sfo sushi\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "ph B-CITY\n",
      "##x O\n",
      "ram O\n",
      "##en O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: phx ramen\n",
      "Predicted entities: phx\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "seattle B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: restaurants seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "in O\n",
      "seattle B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: restaurants in seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "near O\n",
      "seattle B-CITY\n",
      "[SEP] I-CITYSTATE\n",
      "Input: restaurants near seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITY\n",
      "restaurants O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: seattle restaurants\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITYSTATE\n",
      "wa I-CITYSTATE\n",
      "restaurants O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: seattle wa restaurants\n",
      "Predicted entities: seattle wa\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "wa I-CITYSTATE\n",
      "restaurants O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: seattle, wa restaurants\n",
      "Predicted entities: seattle , wa\n",
      "\n",
      "[CLS] O\n",
      "waterloo B-CITYSTATE\n",
      "ia I-CITYSTATE\n",
      "hamburger O\n",
      "##s O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: waterloo ia hamburgers\n",
      "Predicted entities: waterloo ia\n",
      "\n",
      "[CLS] O\n",
      "wal O\n",
      "##m O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: walm\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "foot O\n",
      "##bal O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: footbal\n",
      "Predicted entities: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your text list\n",
    "text_list = [\n",
    "    'New York', 'Los Angeles', 'Chicago', 'Philadelphia', 'Dallas',\n",
    "    'Fort Worth', 'Houston', 'Atlanta', 'Boston', 'Manchester',\n",
    "    'Washington, D.C.', 'Hagerstown', 'San Francisco', 'Oakland',\n",
    "    'San Jose', \n",
    "    'san jose',\n",
    "    'weather in san jose',\n",
    "    'weather in Boston',\n",
    "    'Weather in Boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weather',\n",
    "    'Weather',\n",
    "    'Boston weather',\n",
    "    'Boston Weather',\n",
    "    'I love Pizzahut',\n",
    "    'I like Starbucks',\n",
    "    'sushi restaurants in Sunnyvale, CA',\n",
    "    'sushi restaurants in Sunnyvale, California',\n",
    "    'ramen in sf',\n",
    "    'sushi sf',\n",
    "    'sushi sfo',\n",
    "    'sushi sfo, CA',\n",
    "    'ramen sfo',\n",
    "    'sfo sushi',\n",
    "    'phx ramen',\n",
    "    'restaurants seattle',\n",
    "    'restaurants in seattle',\n",
    "    'restaurants near seattle',\n",
    "    'seattle restaurants',\n",
    "    'seattle wa restaurants',\n",
    "    'seattle, wa restaurants',\n",
    "    'waterloo ia hamburgers',\n",
    "    'walm',\n",
    "    'footbal',\n",
    "]\n",
    "\n",
    "model = trainer.model\n",
    "\n",
    "# Function to make predictions and group entities\n",
    "def predict_ner(text_list):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for text in text_list:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predictions (logits -> predicted labels)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Map the predictions to labels and tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu().numpy())\n",
    "        ner_labels = [model.config.id2label[pred] for pred in predictions]\n",
    "\n",
    "        # Group tokens back into entities\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        entities = []\n",
    "\n",
    "        for token, label in zip(tokens, ner_labels):\n",
    "            print(token, label)\n",
    "            # Ignore special tokens like [CLS], [SEP]\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            # Handle subword tokens (tokens starting with ##)\n",
    "            if token.startswith(\"##\"):\n",
    "                if current_entity:\n",
    "                    current_entity[-1] += token[2:]  # Append the subword without \"##\"\n",
    "            elif label.startswith(\"B-\") or (label.startswith(\"I-\") and label != current_label):\n",
    "                # New entity starts, append the old one\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_entity.append(token)\n",
    "                current_label = label\n",
    "            elif label.startswith(\"I-\") and label == current_label:\n",
    "                # Continue current entity\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                # Non-entity token or 'O'\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "        # Append any remaining entity\n",
    "        if current_entity:\n",
    "            entities.append(\" \".join(current_entity))\n",
    "\n",
    "        # Clean up tokens (remove subword tokens and punctuation issues, etc.)\n",
    "        clean_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.replace(\" ##\", \" \")\n",
    "            entity = entity.replace(\" .\", \".\")  # Handle punctuation\n",
    "            entity = entity.replace(\" ,\", \",\")\n",
    "            clean_entities.append(entity)\n",
    "\n",
    "        # Print the result for comparison\n",
    "        print(f\"Input: {text}\")\n",
    "        print(f\"Predicted entities: {' '.join(clean_entities)}\")\n",
    "        print()\n",
    "\n",
    "# Run predictions on the text list\n",
    "predict_ner(text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdf1c664-80e1-47a1-a33f-98e2dd509623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model (DistilBERT NER model)\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\",\n",
    "                                                             num_labels=11,\n",
    "                                                             id2label=id2label,\n",
    "                                                             label2id=label2id)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Load the LoRA-adapted model\n",
    "peft_config = PeftConfig.from_pretrained(\"results/checkpoint-110628\")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"results/checkpoint-110628\")\n",
    "\n",
    "# Merge the LoRA weights with the base model\n",
    "merged_model = lora_model.merge_and_unload()  # This merges LoRA into the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d63ba4-2659-44b6-bf40-e33cc1516545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tmp/merged_distilbert_uncased_ner/tokenizer_config.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/special_tokens_map.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/vocab.txt',\n",
       " 'tmp/merged_distilbert_uncased_ner/added_tokens.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the merged model and tokenizer\n",
    "save_dir = \"tmp/merged_distilbert_uncased_ner\"\n",
    "merged_model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae28df-7596-4142-9aa8-9e8e5d291f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fbf99-9165-4afe-a34a-a7a112427371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c889aa6-4b3d-479b-8dc7-23abf7a3164e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 265M/265M [00:06<00:00, 39.2MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozilla/distilbert-uncased-NER-LoRA/commit/2812b2b410276357e3526ebbe9cb26479adb7ce5', commit_message='Upload tokenizer', commit_description='', oid='2812b2b410276357e3526ebbe9cb26479adb7ce5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozilla/distilbert-uncased-NER-LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozilla/distilbert-uncased-NER-LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the merged model\n",
    "merged_model_dir = \"tmp/merged_distilbert_uncased_ner\"\n",
    "merged_repo_id = \"Mozilla/distilbert-uncased-NER-LoRA\" \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f08c0-2488-4efb-93b2-2d89a8ff6cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
