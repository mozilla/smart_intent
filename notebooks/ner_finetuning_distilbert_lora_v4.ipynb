{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bbbb75-8c65-440e-a059-c63c2fa91996",
   "metadata": {},
   "source": [
    "purpose of this notebook is to finetune the \"distilbert/distilbert-base-uncased\" model\n",
    "Handles city, state and city-state separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7436463-26a4-4ebb-abd1-771ee134220b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6740344d-7d09-457a-bb68-a64f2b532103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6093b56-0180-4b67-8b04-b562979979ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples_v2.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples_v3.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v2.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v3.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v4.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v5.parquet\")\n",
    "full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v6.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f06a123-bdd7-4655-ae16-92bdc655924f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'id'],\n",
       "    num_rows: 450000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c359f3f-b246-41d0-860c-ba7643a31eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['infinity'], 'ner_tags': [0], 'id': 10}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5b0e6e-e169-4e32-aeae-00c58949ff32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set_size = 5000\n",
    "val_start = len(full_dataset) - val_set_size\n",
    "val_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3d3524-3a60-4ce2-863b-08a6dadd2fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_dataset = full_dataset.select(range(val_start))  # Select training rows\n",
    "val_dataset = full_dataset.select(range(val_start, len(full_dataset)))  # Select last 1000 rows for validation\n",
    "\n",
    "# Combine them into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ffc3cd-886d-4f7f-a8c8-8f5413628646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id'],\n",
       "        num_rows: 445000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340175ec-d224-4c3b-aaa5-16037c61fff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'].to_pandas()['tokens'].apply(len).hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2046667-d9ef-44ab-a444-dc95752478aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f20d0f6-fa8e-47d7-a60b-2e673e25685a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 445000/445000 [01:52<00:00, 3958.86 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4007.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for distilbert-based NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Function to tokenize the input and align labels with tokens\n",
    "def tokenize_and_align_labels(example):\n",
    "    # Tokenize 'tokens' while keeping track of word boundaries\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example['tokens'], \n",
    "        is_split_into_words=True, \n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        max_length=64,\n",
    "    )\n",
    "    \n",
    "    # Get the word_ids (mapping from tokens to original words)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    aligned_labels = []\n",
    "\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)  # Special tokens ([CLS], [SEP], etc.)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(example['ner_tags'][word_idx])  # Assign the label to the first token of each word\n",
    "        else:\n",
    "            aligned_labels.append(-100)  # Subword tokens get label -100\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the function to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106935d0-9093-4ff9-8e0f-afe4aa2d792c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 445000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331d6dca-55d0-473e-94ca-90a152a9e9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['nearest',\n",
       "  'branch',\n",
       "  'of',\n",
       "  'Columbia',\n",
       "  'University',\n",
       "  'in',\n",
       "  'Elgin'],\n",
       " 'ner_tags': [0, 0, 0, 3, 4, 0, 5],\n",
       " 'id': 445000,\n",
       " 'input_ids': [101,\n",
       "  7205,\n",
       "  3589,\n",
       "  1997,\n",
       "  3996,\n",
       "  2118,\n",
       "  1999,\n",
       "  23792,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  5,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1221fd4e-3f16-4ed4-9d9f-c359604545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions_and_labels(predictions, references):\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    cmp_count = 0\n",
    "\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        # Only keep labels that are not -100\n",
    "        true_labels_example = [label for label in reference if label != -100]\n",
    "        \n",
    "        # Align predictions: Remove predictions for which the corresponding reference label is -100\n",
    "        true_predictions_example = [pred for pred, ref in zip(prediction, reference) if ref != -100]\n",
    "\n",
    "        # Ensure the length of predictions and labels matches\n",
    "        if len(true_predictions_example) == len(true_labels_example):\n",
    "            true_labels.append(true_labels_example)\n",
    "            true_predictions.append(true_predictions_example)\n",
    "            cmp_count += 1\n",
    "        else:\n",
    "            # Log or handle the error (example-level mismatch)\n",
    "            # print(f\"Skipping example due to mismatch: predictions ({len(true_predictions_example)}), labels ({len(true_labels_example)})\")\n",
    "            continue  # Skip this example\n",
    "\n",
    "    # Flatten the lists (convert from list of lists to a single list)\n",
    "    true_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
    "    true_labels = [label for sublist in true_labels for label in sublist]\n",
    "    print(f\"cmp_count = {cmp_count} out of {len(predictions)}\")\n",
    "\n",
    "    return true_predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f62dee2-077d-451f-af48-60eaf50e5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Post-process the predictions and labels to remove -100 values\n",
    "    true_predictions, true_labels = postprocess_predictions_and_labels(predictions, labels)\n",
    "\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7acd16-763a-4055-b492-3007b5057da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the NER label mappings\n",
    "# id2label = {\n",
    "#     0: \"O\",         # Outside any entity\n",
    "#     1: \"B-PER\",     # Beginning of a person entity\n",
    "#     2: \"I-PER\",     # Inside a person entity\n",
    "#     3: \"B-ORG\",     # Beginning of an organization entity\n",
    "#     4: \"I-ORG\",     # Inside an organization entity\n",
    "#     5: \"B-LOC\",     # Beginning of a location entity\n",
    "#     6: \"I-LOC\",     # Inside a location entity\n",
    "#     7: \"B-MISC\",    # Beginning of a miscellaneous entity\n",
    "#     8: \"I-MISC\"     # Inside a miscellaneous entity\n",
    "# }\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",        # Outside any named entity\n",
    "    1: \"B-PER\",    # Beginning of a person entity\n",
    "    2: \"I-PER\",    # Inside a person entity\n",
    "    3: \"B-ORG\",    # Beginning of an organization entity\n",
    "    4: \"I-ORG\",    # Inside an organization entity\n",
    "    5: \"B-CITY\",    # Beginning of a city entity\n",
    "    6: \"I-CITY\",    # Inside a city entity\n",
    "    7: \"B-STATE\",    # Beginning of a state entity\n",
    "    8: \"I-STATE\",    # Inside a state entity\n",
    "    9: \"B-CITYSTATE\",   # Beginning of a city_state entity\n",
    "   10: \"I-CITYSTATE\",   # Inside a city_state entity\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", \n",
    "                                                        num_labels=11, \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,  # Task type is token classification (NER)\n",
    "    r=16,  # Low-rank dimension (you can experiment with this)\n",
    "    lora_alpha=32,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    target_modules=['q_lin', 'k_lin']  # LoRA is applied to query layer\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea3559d-4995-4ab9-9f4f-913726f6b906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 303,371 || all params: 66,674,710 || trainable%: 0.4550\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "343c2425-f8a2-4d13-a1cc-22ed6a4717a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c185799-1fd7-4319-a2a5-89ba1ff52df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='166878' max='166878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [166878/166878 2:51:57, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>0.071677</td>\n",
       "      <td>0.269751</td>\n",
       "      <td>0.071677</td>\n",
       "      <td>0.054449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.071870</td>\n",
       "      <td>0.138070</td>\n",
       "      <td>0.071870</td>\n",
       "      <td>0.054404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.072015</td>\n",
       "      <td>0.162980</td>\n",
       "      <td>0.072015</td>\n",
       "      <td>0.055425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.071774</td>\n",
       "      <td>0.128665</td>\n",
       "      <td>0.071774</td>\n",
       "      <td>0.055241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.070904</td>\n",
       "      <td>0.204738</td>\n",
       "      <td>0.070904</td>\n",
       "      <td>0.054977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.070952</td>\n",
       "      <td>0.189633</td>\n",
       "      <td>0.070952</td>\n",
       "      <td>0.054714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4053 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4053 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4053 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4053 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4053 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4053 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=166878, training_loss=0.027890973929543726, metrics={'train_runtime': 10325.0473, 'train_samples_per_second': 258.594, 'train_steps_per_second': 16.162, 'total_flos': 4.392360831744e+16, 'train_loss': 0.027890973929543726, 'epoch': 6.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Output directory\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate at the end of every epoch\n",
    "    learning_rate=2e-5,               # Learning rate\n",
    "    per_device_train_batch_size=16,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
    "    num_train_epochs=6,               # Number of training epochs\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    logging_dir='./logs',             # Directory for logging\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,                  # LoRA-wrapped model\n",
    "    args=training_args,                # Training arguments\n",
    "    train_dataset=tokenized_dataset['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],  # Validation dataset (if available)\n",
    "    tokenizer=tokenizer,               # Tokenizer\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82880e92-4d83-442a-9a1c-3a2386b1c942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] O\n",
      "new B-STATE\n",
      "york I-STATE\n",
      "[SEP] B-PER\n",
      "Input: New York\n",
      "Predicted entities: new york\n",
      "\n",
      "[CLS] O\n",
      "los B-CITY\n",
      "angeles I-CITY\n",
      "[SEP] B-CITY\n",
      "Input: Los Angeles\n",
      "Predicted entities: los angeles\n",
      "\n",
      "[CLS] B-STATE\n",
      "chicago B-CITY\n",
      "[SEP] B-PER\n",
      "Input: Chicago\n",
      "Predicted entities: chicago\n",
      "\n",
      "[CLS] O\n",
      "philadelphia B-CITY\n",
      "[SEP] B-ORG\n",
      "Input: Philadelphia\n",
      "Predicted entities: philadelphia\n",
      "\n",
      "[CLS] O\n",
      "dallas B-CITY\n",
      "[SEP] B-ORG\n",
      "Input: Dallas\n",
      "Predicted entities: dallas\n",
      "\n",
      "[CLS] O\n",
      "fort B-CITY\n",
      "worth I-CITY\n",
      "[SEP] B-ORG\n",
      "Input: Fort Worth\n",
      "Predicted entities: fort worth\n",
      "\n",
      "[CLS] O\n",
      "houston B-CITY\n",
      "[SEP] B-ORG\n",
      "Input: Houston\n",
      "Predicted entities: houston\n",
      "\n",
      "[CLS] O\n",
      "atlanta B-CITY\n",
      "[SEP] B-ORG\n",
      "Input: Atlanta\n",
      "Predicted entities: atlanta\n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "[SEP] B-PER\n",
      "Input: Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "manchester B-CITY\n",
      "[SEP] B-ORG\n",
      "Input: Manchester\n",
      "Predicted entities: manchester\n",
      "\n",
      "[CLS] O\n",
      "washington B-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "d I-CITYSTATE\n",
      ". I-CITYSTATE\n",
      "c I-CITYSTATE\n",
      ". I-CITY\n",
      "[SEP] B-PER\n",
      "Input: Washington, D.C.\n",
      "Predicted entities: washington , d. c .\n",
      "\n",
      "[CLS] O\n",
      "ha B-CITY\n",
      "##gers B-CITY\n",
      "##town I-CITY\n",
      "[SEP] B-CITY\n",
      "Input: Hagerstown\n",
      "Predicted entities: hagerstown\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "francisco I-CITY\n",
      "[SEP] B-ORG\n",
      "Input: San Francisco\n",
      "Predicted entities: san francisco\n",
      "\n",
      "[CLS] O\n",
      "oakland B-CITY\n",
      "[SEP] B-ORG\n",
      "Input: Oakland\n",
      "Predicted entities: oakland\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] B-ORG\n",
      "Input: San Jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] B-ORG\n",
      "Input: san jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] B-CITY\n",
      "Input: weather in san jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "boston B-CITY\n",
      "[SEP] B-PER\n",
      "Input: weather in Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "boston B-CITY\n",
      "[SEP] B-PER\n",
      "Input: Weather in Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "boston B-CITY\n",
      "[SEP] B-PER\n",
      "Input: weather Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "boston B-CITY\n",
      "[SEP] B-PER\n",
      "Input: Weather Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] B-CITY\n",
      "Input: weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] B-CITY\n",
      "Input: Weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "weather O\n",
      "[SEP] B-PER\n",
      "Input: Boston weather\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "weather O\n",
      "[SEP] B-PER\n",
      "Input: Boston Weather\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "i O\n",
      "love O\n",
      "pizza O\n",
      "##hu B-CITY\n",
      "##t O\n",
      "[SEP] O\n",
      "Input: I love Pizzahut\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "i O\n",
      "like O\n",
      "starbucks O\n",
      "[SEP] B-PER\n",
      "Input: I like Starbucks\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "restaurants O\n",
      "in O\n",
      "sunny B-CITYSTATE\n",
      "##vale I-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "ca I-CITYSTATE\n",
      "[SEP] B-CITY\n",
      "Input: sushi restaurants in Sunnyvale, CA\n",
      "Predicted entities: sunnyvale , ca\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "restaurants O\n",
      "in O\n",
      "sunny B-CITYSTATE\n",
      "##vale I-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "california I-CITYSTATE\n",
      "[SEP] B-CITY\n",
      "Input: sushi restaurants in Sunnyvale, California\n",
      "Predicted entities: sunnyvale , california\n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "in O\n",
      "sf B-CITY\n",
      "[SEP] O\n",
      "Input: ramen in sf\n",
      "Predicted entities: sf\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITY\n",
      "[SEP] B-CITY\n",
      "Input: sushi sf\n",
      "Predicted entities: sf\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITY\n",
      "##o O\n",
      "[SEP] B-CITY\n",
      "Input: sushi sfo\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITYSTATE\n",
      "##o I-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "ca I-CITYSTATE\n",
      "[SEP] B-PER\n",
      "Input: sushi sfo, CA\n",
      "Predicted entities: sfo , ca\n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "sf B-CITY\n",
      "##o O\n",
      "[SEP] B-CITY\n",
      "Input: ramen sfo\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "sf B-CITY\n",
      "##o O\n",
      "su O\n",
      "##shi O\n",
      "[SEP] O\n",
      "Input: sfo sushi\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "ph B-CITY\n",
      "##x O\n",
      "ram O\n",
      "##en O\n",
      "[SEP] O\n",
      "Input: phx ramen\n",
      "Predicted entities: phx\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "seattle B-CITY\n",
      "[SEP] B-ORG\n",
      "Input: restaurants seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "in O\n",
      "seattle B-CITY\n",
      "[SEP] B-ORG\n",
      "Input: restaurants in seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "near O\n",
      "seattle B-CITY\n",
      "[SEP] B-CITY\n",
      "Input: restaurants near seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITY\n",
      "restaurants O\n",
      "[SEP] B-ORG\n",
      "Input: seattle restaurants\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITYSTATE\n",
      "wa I-CITYSTATE\n",
      "restaurants O\n",
      "[SEP] B-CITYSTATE\n",
      "Input: seattle wa restaurants\n",
      "Predicted entities: seattle wa\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "wa I-CITYSTATE\n",
      "restaurants O\n",
      "[SEP] B-PER\n",
      "Input: seattle, wa restaurants\n",
      "Predicted entities: seattle , wa\n",
      "\n",
      "[CLS] O\n",
      "waterloo B-CITYSTATE\n",
      "ia I-CITYSTATE\n",
      "hamburger O\n",
      "##s O\n",
      "[SEP] B-PER\n",
      "Input: waterloo ia hamburgers\n",
      "Predicted entities: waterloo ia\n",
      "\n",
      "[CLS] O\n",
      "best O\n",
      "smartphone O\n",
      "[SEP] O\n",
      "Input: best smartphone\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITY\n",
      "[SEP] B-CITY\n",
      "Input: sushi sf\n",
      "Predicted entities: sf\n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "sf B-CITY\n",
      "[SEP] B-CITY\n",
      "Input: ramen sf\n",
      "Predicted entities: sf\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "[SEP] B-CITY\n",
      "Input: sushi\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "[SEP] B-CITY\n",
      "Input: ramen\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "foot O\n",
      "##bal O\n",
      "[SEP] O\n",
      "Input: footbal\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "breaking O\n",
      "bad O\n",
      "[SEP] B-PER\n",
      "Input: breaking bad\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "wal B-CITY\n",
      "##m I-CITY\n",
      "[SEP] B-ORG\n",
      "Input: walm\n",
      "Predicted entities: walm\n",
      "\n",
      "[CLS] O\n",
      "foot O\n",
      "##bal O\n",
      "[SEP] O\n",
      "Input: footbal\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "ra B-STATE\n",
      "[SEP] B-CITY\n",
      "Input: ramen ra\n",
      "Predicted entities: ra\n",
      "\n",
      "[CLS] O\n",
      "big O\n",
      "city O\n",
      "[SEP] B-PER\n",
      "Input: big city\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] B-STATE\n",
      "orlando B-PER\n",
      "bloom I-PER\n",
      "[SEP] B-ORG\n",
      "Input: orlando bloom\n",
      "Predicted entities: orlando bloom\n",
      "\n",
      "[CLS] O\n",
      "banana O\n",
      "[SEP] O\n",
      "Input: banana\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "len B-CITY\n",
      "##ght I-CITY\n",
      "[SEP] O\n",
      "Input: lenght\n",
      "Predicted entities: lenght\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "in O\n",
      "abc B-CITY\n",
      "##id I-CITY\n",
      "[SEP] O\n",
      "Input: restaurants in abcid\n",
      "Predicted entities: abcid\n",
      "\n",
      "[CLS] O\n",
      "allie B-CITY\n",
      "##n I-CITY\n",
      "[SEP] B-CITY\n",
      "Input: allien\n",
      "Predicted entities: allien\n",
      "\n",
      "[CLS] O\n",
      "prop O\n",
      "##osa O\n",
      "[SEP] B-ORG\n",
      "Input: proposa\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "wat O\n",
      "##her O\n",
      "[SEP] O\n",
      "Input: wather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "orlando B-CITY\n",
      "real I-ORG\n",
      "##ty I-ORG\n",
      "group O\n",
      "[SEP] B-ORG\n",
      "Input: orlando realty group\n",
      "Predicted entities: orlando realty\n",
      "\n",
      "[CLS] O\n",
      "coffee O\n",
      "near O\n",
      "sunny B-CITY\n",
      "##val I-CITY\n",
      "[SEP] B-CITY\n",
      "Input: coffee near sunnyval\n",
      "Predicted entities: sunnyval\n",
      "\n",
      "[CLS] O\n",
      "coffee O\n",
      "near O\n",
      "me O\n",
      "sunny B-CITY\n",
      "##val I-CITY\n",
      "[SEP] O\n",
      "Input: coffee near me sunnyval\n",
      "Predicted entities: sunnyval\n",
      "\n",
      "[CLS] O\n",
      "coffee O\n",
      "near O\n",
      "me O\n",
      "sunny B-CITY\n",
      "##vale I-CITY\n",
      "[SEP] B-CITY\n",
      "Input: coffee near me sunnyvale\n",
      "Predicted entities: sunnyvale\n",
      "\n",
      "[CLS] O\n",
      "coffee O\n",
      "near O\n",
      "me O\n",
      "sunny B-CITY\n",
      "##va I-CITY\n",
      "[SEP] B-CITY\n",
      "Input: coffee near me sunnyva\n",
      "Predicted entities: sunnyva\n",
      "\n",
      "[CLS] O\n",
      "top O\n",
      "gun O\n",
      "maverick B-CITY\n",
      "[SEP] B-PER\n",
      "Input: top gun maverick\n",
      "Predicted entities: maverick\n",
      "\n",
      "[CLS] O\n",
      "clean O\n",
      "pa O\n",
      "##pe O\n",
      "[SEP] B-PER\n",
      "Input: clean pape\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "paws O\n",
      "sc O\n",
      "##or O\n",
      "[SEP] B-PER\n",
      "Input: paws scor\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "cr B-PER\n",
      "##ist O\n",
      "##iano B-PER\n",
      "ronald I-PER\n",
      "##o I-PER\n",
      "[SEP] I-CITYSTATE\n",
      "Input: cristiano ronaldo\n",
      "Predicted entities: cristiano ronaldo\n",
      "\n",
      "[CLS] O\n",
      "fast O\n",
      "##fo O\n",
      "##od O\n",
      "[SEP] O\n",
      "Input: fastfood\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "int B-CITY\n",
      "##e B-CITY\n",
      "[SEP] O\n",
      "Input: inte\n",
      "Predicted entities: inte\n",
      "\n",
      "[CLS] O\n",
      "potato O\n",
      "##s O\n",
      "[SEP] O\n",
      "Input: potatos\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "vision O\n",
      "##are O\n",
      "[SEP] B-PER\n",
      "Input: visionare\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "allie B-CITY\n",
      "##n I-CITY\n",
      "[SEP] B-CITY\n",
      "Input: allien\n",
      "Predicted entities: allien\n",
      "\n",
      "[CLS] O\n",
      "prop O\n",
      "##osa O\n",
      "[SEP] B-ORG\n",
      "Input: proposa\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "wat O\n",
      "##her O\n",
      "[SEP] O\n",
      "Input: wather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "big O\n",
      "city O\n",
      "[SEP] B-PER\n",
      "Input: big city\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] B-STATE\n",
      "orlando B-PER\n",
      "bloom I-PER\n",
      "[SEP] B-ORG\n",
      "Input: orlando bloom\n",
      "Predicted entities: orlando bloom\n",
      "\n",
      "[CLS] O\n",
      "banana O\n",
      "[SEP] O\n",
      "Input: banana\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "len B-CITY\n",
      "##ght I-CITY\n",
      "[SEP] O\n",
      "Input: lenght\n",
      "Predicted entities: lenght\n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "ra B-STATE\n",
      "[SEP] B-CITY\n",
      "Input: ramen ra\n",
      "Predicted entities: ra\n",
      "\n",
      "[CLS] O\n",
      "cafe O\n",
      "ca B-STATE\n",
      "[SEP] B-PER\n",
      "Input: cafe ca\n",
      "Predicted entities: ca\n",
      "\n",
      "[CLS] O\n",
      "arcade O\n",
      "[SEP] O\n",
      "Input: arcade\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "bag O\n",
      "##els O\n",
      "[SEP] O\n",
      "Input: bagels\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "bank B-ORG\n",
      "of O\n",
      "america I-STATE\n",
      "[SEP] B-ORG\n",
      "Input: bank of america\n",
      "Predicted entities: bank america\n",
      "\n",
      "[CLS] O\n",
      "bowling O\n",
      "[SEP] O\n",
      "Input: bowling\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "cup O\n",
      "##cake O\n",
      "##s O\n",
      "[SEP] B-CITY\n",
      "Input: cupcakes\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] B-CITY\n",
      "electric O\n",
      "##ian O\n",
      "[SEP] I-CITYSTATE\n",
      "Input: electrician\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "kara O\n",
      "##oke O\n",
      "[SEP] B-CITY\n",
      "Input: karaoke\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "korean O\n",
      "bb O\n",
      "##q O\n",
      "[SEP] B-CITY\n",
      "Input: korean bbq\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "laser O\n",
      "tag O\n",
      "[SEP] B-PER\n",
      "Input: laser tag\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "pi O\n",
      "##lates O\n",
      "[SEP] B-PER\n",
      "Input: pilates\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "sports O\n",
      "bar O\n",
      "[SEP] O\n",
      "Input: sports bar\n",
      "Predicted entities: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your text list\n",
    "text_list = [\n",
    "    'New York', 'Los Angeles', 'Chicago', 'Philadelphia', 'Dallas',\n",
    "    'Fort Worth', 'Houston', 'Atlanta', 'Boston', 'Manchester',\n",
    "    'Washington, D.C.', 'Hagerstown', 'San Francisco', 'Oakland',\n",
    "    'San Jose', \n",
    "    'san jose',\n",
    "    'weather in san jose',\n",
    "    'weather in Boston',\n",
    "    'Weather in Boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weather',\n",
    "    'Weather',\n",
    "    'Boston weather',\n",
    "    'Boston Weather',\n",
    "    'I love Pizzahut',\n",
    "    'I like Starbucks',\n",
    "    'sushi restaurants in Sunnyvale, CA',\n",
    "    'sushi restaurants in Sunnyvale, California',\n",
    "    'ramen in sf',\n",
    "    'sushi sf',\n",
    "    'sushi sfo',\n",
    "    'sushi sfo, CA',\n",
    "    'ramen sfo',\n",
    "    'sfo sushi',\n",
    "    'phx ramen',\n",
    "    'restaurants seattle',\n",
    "    'restaurants in seattle',\n",
    "    'restaurants near seattle',\n",
    "    'seattle restaurants',\n",
    "    'seattle wa restaurants',\n",
    "    'seattle, wa restaurants',\n",
    "    'waterloo ia hamburgers',\n",
    "    'best smartphone',\n",
    "    'sushi sf',\n",
    "    'ramen sf',\n",
    "    'sushi',\n",
    "    'ramen',\n",
    "    'footbal',\n",
    "    'breaking bad',\n",
    "    'walm',\n",
    "    'footbal',\n",
    "    'ramen ra',\n",
    "    'big city',\n",
    "    'orlando bloom', \n",
    "    'banana', \n",
    "    'lenght',\n",
    "    'restaurants in abcid',\n",
    "    'allien', 'proposa', 'wather',\n",
    "    'orlando realty group',\n",
    "    'coffee near sunnyval',\n",
    "    'coffee near me sunnyval',\n",
    "    'coffee near me sunnyvale',\n",
    "    'coffee near me sunnyva',\n",
    "    'top gun maverick','clean pape','paws scor',\n",
    "    'cristiano ronaldo', 'fastfood', 'inte', 'potatos', 'visionare',\n",
    "    'allien', 'proposa', 'wather', 'big city', 'orlando bloom', 'banana', 'lenght',\n",
    "    'ramen ra', 'cafe ca',\n",
    "    'arcade',\n",
    "    'bagels',\n",
    "    'bank of america',\n",
    "    'bowling',\n",
    "    'cupcakes',\n",
    "    'electrician',\n",
    "    'karaoke',\n",
    "    'korean bbq',\n",
    "    'laser tag',\n",
    "    'pilates',\n",
    "    'sports bar',\n",
    "]\n",
    "\n",
    "model = trainer.model\n",
    "\n",
    "\n",
    "# Function to make predictions and group entities\n",
    "def predict_ner(text_list):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for text in text_list:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predictions (logits -> predicted labels)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Map the predictions to labels and tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu().numpy())\n",
    "        ner_labels = [model.config.id2label[pred] for pred in predictions]\n",
    "\n",
    "        # Group tokens back into entities\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        entities = []\n",
    "\n",
    "        for token, label in zip(tokens, ner_labels):\n",
    "            print(token, label)\n",
    "            # Ignore special tokens like [CLS], [SEP]\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            # Handle subword tokens (tokens starting with ##)\n",
    "            if token.startswith(\"##\"):\n",
    "                if current_entity:\n",
    "                    current_entity[-1] += token[2:]  # Append the subword without \"##\"\n",
    "            elif label.startswith(\"B-\") or (label.startswith(\"I-\") and label != current_label):\n",
    "                # New entity starts, append the old one\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_entity.append(token)\n",
    "                current_label = label\n",
    "            elif label.startswith(\"I-\") and label == current_label:\n",
    "                # Continue current entity\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                # Non-entity token or 'O'\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "        # Append any remaining entity\n",
    "        if current_entity:\n",
    "            entities.append(\" \".join(current_entity))\n",
    "\n",
    "        # Clean up tokens (remove subword tokens and punctuation issues, etc.)\n",
    "        clean_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.replace(\" ##\", \" \")\n",
    "            entity = entity.replace(\" .\", \".\")  # Handle punctuation\n",
    "            entity = entity.replace(\" ,\", \",\")\n",
    "            clean_entities.append(entity)\n",
    "\n",
    "        # Print the result for comparison\n",
    "        print(f\"Input: {text}\")\n",
    "        print(f\"Predicted entities: {' '.join(clean_entities)}\")\n",
    "        print()\n",
    "\n",
    "# Run predictions on the text list\n",
    "predict_ner(text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdf1c664-80e1-47a1-a33f-98e2dd509623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model (DistilBERT NER model)\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\",\n",
    "                                                             num_labels=11,\n",
    "                                                             id2label=id2label,\n",
    "                                                             label2id=label2id)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Load the LoRA-adapted model\n",
    "peft_config = PeftConfig.from_pretrained(\"results/checkpoint-166878\")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"results/checkpoint-166878\")\n",
    "\n",
    "# Merge the LoRA weights with the base model\n",
    "merged_model = lora_model.merge_and_unload()  # This merges LoRA into the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9d63ba4-2659-44b6-bf40-e33cc1516545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tmp/merged_distilbert_uncased_ner/tokenizer_config.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/special_tokens_map.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/vocab.txt',\n",
       " 'tmp/merged_distilbert_uncased_ner/added_tokens.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the merged model and tokenizer\n",
    "save_dir = \"tmp/merged_distilbert_uncased_ner\"\n",
    "merged_model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efae28df-7596-4142-9aa8-9e8e5d291f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b73fbf99-9165-4afe-a34a-a7a112427371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c889aa6-4b3d-479b-8dc7-23abf7a3164e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 265M/265M [00:05<00:00, 44.3MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozilla/distilbert-uncased-NER-LoRA/commit/8c51147d4354c83a8a582200bf271dfd81932a31', commit_message='Upload tokenizer', commit_description='', oid='8c51147d4354c83a8a582200bf271dfd81932a31', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozilla/distilbert-uncased-NER-LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozilla/distilbert-uncased-NER-LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the merged model\n",
    "merged_model_dir = \"tmp/merged_distilbert_uncased_ner\"\n",
    "merged_repo_id = \"Mozilla/distilbert-uncased-NER-LoRA\" \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f08c0-2488-4efb-93b2-2d89a8ff6cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
