{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bbbb75-8c65-440e-a059-c63c2fa91996",
   "metadata": {},
   "source": [
    "purpose of this notebook is to finetune the \"distilbert/distilbert-base-uncased\" model\n",
    "Handles city, state and city-state separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7436463-26a4-4ebb-abd1-771ee134220b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6740344d-7d09-457a-bb68-a64f2b532103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6093b56-0180-4b67-8b04-b562979979ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 400000 examples [00:00, 1201777.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples_v2.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples_v3.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v2.parquet\")\n",
    "# full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v3.parquet\")\n",
    "full_dataset = Dataset.from_parquet(\"data/synthetic_loc_dataset_v4.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f06a123-bdd7-4655-ae16-92bdc655924f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'id'],\n",
       "    num_rows: 400000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5b0e6e-e169-4e32-aeae-00c58949ff32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set_size = 5000\n",
    "val_start = len(full_dataset) - val_set_size\n",
    "val_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3d3524-3a60-4ce2-863b-08a6dadd2fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_dataset = full_dataset.select(range(val_start))  # Select training rows\n",
    "val_dataset = full_dataset.select(range(val_start, len(full_dataset)))  # Select last 1000 rows for validation\n",
    "\n",
    "# Combine them into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ffc3cd-886d-4f7f-a8c8-8f5413628646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id'],\n",
       "        num_rows: 395000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340175ec-d224-4c3b-aaa5-16037c61fff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'].to_pandas()['tokens'].apply(len).hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2046667-d9ef-44ab-a444-dc95752478aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f20d0f6-fa8e-47d7-a60b-2e673e25685a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 395000/395000 [01:43<00:00, 3807.98 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3874.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for distilbert-based NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Function to tokenize the input and align labels with tokens\n",
    "def tokenize_and_align_labels(example):\n",
    "    # Tokenize 'tokens' while keeping track of word boundaries\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example['tokens'], \n",
    "        is_split_into_words=True, \n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        max_length=64,\n",
    "    )\n",
    "    \n",
    "    # Get the word_ids (mapping from tokens to original words)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    aligned_labels = []\n",
    "\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)  # Special tokens ([CLS], [SEP], etc.)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(example['ner_tags'][word_idx])  # Assign the label to the first token of each word\n",
    "        else:\n",
    "            aligned_labels.append(-100)  # Subword tokens get label -100\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the function to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106935d0-9093-4ff9-8e0f-afe4aa2d792c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 395000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331d6dca-55d0-473e-94ca-90a152a9e9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['soups', 'in', 'Nevada'],\n",
       " 'ner_tags': [0, 0, 7],\n",
       " 'id': 395000,\n",
       " 'input_ids': [101,\n",
       "  11350,\n",
       "  2015,\n",
       "  1999,\n",
       "  7756,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  7,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1221fd4e-3f16-4ed4-9d9f-c359604545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions_and_labels(predictions, references):\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    cmp_count = 0\n",
    "\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        # Only keep labels that are not -100\n",
    "        true_labels_example = [label for label in reference if label != -100]\n",
    "        \n",
    "        # Align predictions: Remove predictions for which the corresponding reference label is -100\n",
    "        true_predictions_example = [pred for pred, ref in zip(prediction, reference) if ref != -100]\n",
    "\n",
    "        # Ensure the length of predictions and labels matches\n",
    "        if len(true_predictions_example) == len(true_labels_example):\n",
    "            true_labels.append(true_labels_example)\n",
    "            true_predictions.append(true_predictions_example)\n",
    "            cmp_count += 1\n",
    "        else:\n",
    "            # Log or handle the error (example-level mismatch)\n",
    "            # print(f\"Skipping example due to mismatch: predictions ({len(true_predictions_example)}), labels ({len(true_labels_example)})\")\n",
    "            continue  # Skip this example\n",
    "\n",
    "    # Flatten the lists (convert from list of lists to a single list)\n",
    "    true_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
    "    true_labels = [label for sublist in true_labels for label in sublist]\n",
    "    print(f\"cmp_count = {cmp_count} out of {len(predictions)}\")\n",
    "\n",
    "    return true_predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f62dee2-077d-451f-af48-60eaf50e5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Post-process the predictions and labels to remove -100 values\n",
    "    true_predictions, true_labels = postprocess_predictions_and_labels(predictions, labels)\n",
    "\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7acd16-763a-4055-b492-3007b5057da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the NER label mappings\n",
    "# id2label = {\n",
    "#     0: \"O\",         # Outside any entity\n",
    "#     1: \"B-PER\",     # Beginning of a person entity\n",
    "#     2: \"I-PER\",     # Inside a person entity\n",
    "#     3: \"B-ORG\",     # Beginning of an organization entity\n",
    "#     4: \"I-ORG\",     # Inside an organization entity\n",
    "#     5: \"B-LOC\",     # Beginning of a location entity\n",
    "#     6: \"I-LOC\",     # Inside a location entity\n",
    "#     7: \"B-MISC\",    # Beginning of a miscellaneous entity\n",
    "#     8: \"I-MISC\"     # Inside a miscellaneous entity\n",
    "# }\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",        # Outside any named entity\n",
    "    1: \"B-PER\",    # Beginning of a person entity\n",
    "    2: \"I-PER\",    # Inside a person entity\n",
    "    3: \"B-ORG\",    # Beginning of an organization entity\n",
    "    4: \"I-ORG\",    # Inside an organization entity\n",
    "    5: \"B-CITY\",    # Beginning of a city entity\n",
    "    6: \"I-CITY\",    # Inside a city entity\n",
    "    7: \"B-STATE\",    # Beginning of a state entity\n",
    "    8: \"I-STATE\",    # Inside a state entity\n",
    "    9: \"B-CITYSTATE\",   # Beginning of a city_state entity\n",
    "   10: \"I-CITYSTATE\",   # Inside a city_state entity\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", \n",
    "                                                        num_labels=11, \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,  # Task type is token classification (NER)\n",
    "    r=16,  # Low-rank dimension (you can experiment with this)\n",
    "    lora_alpha=32,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    target_modules=['q_lin', 'k_lin']  # LoRA is applied to query layer\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea3559d-4995-4ab9-9f4f-913726f6b906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 303,371 || all params: 66,674,710 || trainable%: 0.4550\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "343c2425-f8a2-4d13-a1cc-22ed6a4717a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c185799-1fd7-4319-a2a5-89ba1ff52df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='148128' max='148128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [148128/148128 2:32:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>0.490154</td>\n",
       "      <td>0.084216</td>\n",
       "      <td>0.091639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.083119</td>\n",
       "      <td>0.507702</td>\n",
       "      <td>0.083119</td>\n",
       "      <td>0.090871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.075961</td>\n",
       "      <td>0.510936</td>\n",
       "      <td>0.075961</td>\n",
       "      <td>0.079411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.080542</td>\n",
       "      <td>0.501463</td>\n",
       "      <td>0.080542</td>\n",
       "      <td>0.090567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.075818</td>\n",
       "      <td>0.506821</td>\n",
       "      <td>0.075818</td>\n",
       "      <td>0.082395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.075150</td>\n",
       "      <td>0.506318</td>\n",
       "      <td>0.075150</td>\n",
       "      <td>0.080811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4075 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4075 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4075 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4075 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4075 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 4075 out of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=148128, training_loss=0.029309118492901003, metrics={'train_runtime': 9130.7796, 'train_samples_per_second': 259.562, 'train_steps_per_second': 16.223, 'total_flos': 3.898837142784e+16, 'train_loss': 0.029309118492901003, 'epoch': 6.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Output directory\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate at the end of every epoch\n",
    "    learning_rate=2e-5,               # Learning rate\n",
    "    per_device_train_batch_size=16,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
    "    num_train_epochs=6,               # Number of training epochs\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    logging_dir='./logs',             # Directory for logging\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,                  # LoRA-wrapped model\n",
    "    args=training_args,                # Training arguments\n",
    "    train_dataset=tokenized_dataset['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],  # Validation dataset (if available)\n",
    "    tokenizer=tokenizer,               # Tokenizer\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82880e92-4d83-442a-9a1c-3a2386b1c942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] O\n",
      "new B-STATE\n",
      "york I-STATE\n",
      "[SEP] O\n",
      "Input: New York\n",
      "Predicted entities: new york\n",
      "\n",
      "[CLS] O\n",
      "los B-CITY\n",
      "angeles I-CITY\n",
      "[SEP] O\n",
      "Input: Los Angeles\n",
      "Predicted entities: los angeles\n",
      "\n",
      "[CLS] O\n",
      "chicago B-CITY\n",
      "[SEP] O\n",
      "Input: Chicago\n",
      "Predicted entities: chicago\n",
      "\n",
      "[CLS] O\n",
      "philadelphia B-CITY\n",
      "[SEP] O\n",
      "Input: Philadelphia\n",
      "Predicted entities: philadelphia\n",
      "\n",
      "[CLS] O\n",
      "dallas B-CITY\n",
      "[SEP] O\n",
      "Input: Dallas\n",
      "Predicted entities: dallas\n",
      "\n",
      "[CLS] O\n",
      "fort B-CITY\n",
      "worth I-CITY\n",
      "[SEP] O\n",
      "Input: Fort Worth\n",
      "Predicted entities: fort worth\n",
      "\n",
      "[CLS] O\n",
      "houston B-CITY\n",
      "[SEP] O\n",
      "Input: Houston\n",
      "Predicted entities: houston\n",
      "\n",
      "[CLS] O\n",
      "atlanta B-CITY\n",
      "[SEP] O\n",
      "Input: Atlanta\n",
      "Predicted entities: atlanta\n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "[SEP] O\n",
      "Input: Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "manchester B-CITY\n",
      "[SEP] O\n",
      "Input: Manchester\n",
      "Predicted entities: manchester\n",
      "\n",
      "[CLS] O\n",
      "washington B-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "d I-CITYSTATE\n",
      ". I-CITYSTATE\n",
      "c I-CITYSTATE\n",
      ". I-CITY\n",
      "[SEP] O\n",
      "Input: Washington, D.C.\n",
      "Predicted entities: washington , d. c .\n",
      "\n",
      "[CLS] O\n",
      "ha B-CITY\n",
      "##gers B-CITY\n",
      "##town I-CITY\n",
      "[SEP] O\n",
      "Input: Hagerstown\n",
      "Predicted entities: hagerstown\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "francisco I-CITY\n",
      "[SEP] O\n",
      "Input: San Francisco\n",
      "Predicted entities: san francisco\n",
      "\n",
      "[CLS] O\n",
      "oakland B-CITY\n",
      "[SEP] O\n",
      "Input: Oakland\n",
      "Predicted entities: oakland\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] O\n",
      "Input: San Jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] O\n",
      "Input: san jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "san B-CITY\n",
      "jose I-CITY\n",
      "[SEP] O\n",
      "Input: weather in san jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "boston B-CITY\n",
      "[SEP] O\n",
      "Input: weather in Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "boston B-CITY\n",
      "[SEP] O\n",
      "Input: Weather in Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "boston B-CITY\n",
      "[SEP] O\n",
      "Input: weather Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "boston B-CITY\n",
      "[SEP] O\n",
      "Input: Weather Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: Weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: Boston weather\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "boston B-CITY\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: Boston Weather\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "i O\n",
      "love O\n",
      "pizza O\n",
      "##hu B-CITY\n",
      "##t O\n",
      "[SEP] O\n",
      "Input: I love Pizzahut\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "i O\n",
      "like O\n",
      "starbucks O\n",
      "[SEP] O\n",
      "Input: I like Starbucks\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "restaurants O\n",
      "in O\n",
      "sunny B-CITYSTATE\n",
      "##vale I-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "ca I-CITYSTATE\n",
      "[SEP] O\n",
      "Input: sushi restaurants in Sunnyvale, CA\n",
      "Predicted entities: sunnyvale , ca\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "restaurants O\n",
      "in O\n",
      "sunny B-CITYSTATE\n",
      "##vale I-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "california I-CITYSTATE\n",
      "[SEP] O\n",
      "Input: sushi restaurants in Sunnyvale, California\n",
      "Predicted entities: sunnyvale , california\n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "in O\n",
      "sf B-CITY\n",
      "[SEP] O\n",
      "Input: ramen in sf\n",
      "Predicted entities: sf\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITY\n",
      "[SEP] O\n",
      "Input: sushi sf\n",
      "Predicted entities: sf\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITY\n",
      "##o O\n",
      "[SEP] O\n",
      "Input: sushi sfo\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "su O\n",
      "##shi O\n",
      "sf B-CITYSTATE\n",
      "##o I-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "ca I-CITYSTATE\n",
      "[SEP] O\n",
      "Input: sushi sfo, CA\n",
      "Predicted entities: sfo , ca\n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "sf B-CITY\n",
      "##o O\n",
      "[SEP] O\n",
      "Input: ramen sfo\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "sf B-CITY\n",
      "##o O\n",
      "su O\n",
      "##shi O\n",
      "[SEP] O\n",
      "Input: sfo sushi\n",
      "Predicted entities: sfo\n",
      "\n",
      "[CLS] O\n",
      "ph B-CITY\n",
      "##x O\n",
      "ram O\n",
      "##en O\n",
      "[SEP] O\n",
      "Input: phx ramen\n",
      "Predicted entities: phx\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "seattle B-CITY\n",
      "[SEP] O\n",
      "Input: restaurants seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "in O\n",
      "seattle B-CITY\n",
      "[SEP] O\n",
      "Input: restaurants in seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "near O\n",
      "seattle B-CITY\n",
      "[SEP] O\n",
      "Input: restaurants near seattle\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITY\n",
      "restaurants O\n",
      "[SEP] O\n",
      "Input: seattle restaurants\n",
      "Predicted entities: seattle\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITYSTATE\n",
      "wa I-CITYSTATE\n",
      "restaurants O\n",
      "[SEP] O\n",
      "Input: seattle wa restaurants\n",
      "Predicted entities: seattle wa\n",
      "\n",
      "[CLS] O\n",
      "seattle B-CITYSTATE\n",
      ", I-CITYSTATE\n",
      "wa I-CITYSTATE\n",
      "restaurants O\n",
      "[SEP] O\n",
      "Input: seattle, wa restaurants\n",
      "Predicted entities: seattle , wa\n",
      "\n",
      "[CLS] O\n",
      "waterloo B-CITYSTATE\n",
      "ia I-CITYSTATE\n",
      "hamburger O\n",
      "##s O\n",
      "[SEP] O\n",
      "Input: waterloo ia hamburgers\n",
      "Predicted entities: waterloo ia\n",
      "\n",
      "[CLS] O\n",
      "wal O\n",
      "##m I-CITY\n",
      "[SEP] O\n",
      "Input: walm\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "foot O\n",
      "##bal O\n",
      "[SEP] O\n",
      "Input: footbal\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "ram O\n",
      "##en O\n",
      "ra B-CITY\n",
      "[SEP] O\n",
      "Input: ramen ra\n",
      "Predicted entities: ra\n",
      "\n",
      "[CLS] O\n",
      "big O\n",
      "city O\n",
      "[SEP] O\n",
      "Input: big city\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] B-STATE\n",
      "orlando B-PER\n",
      "bloom I-PER\n",
      "[SEP] O\n",
      "Input: orlando bloom\n",
      "Predicted entities: orlando bloom\n",
      "\n",
      "[CLS] O\n",
      "banana O\n",
      "[SEP] O\n",
      "Input: banana\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "len O\n",
      "##ght B-CITY\n",
      "[SEP] O\n",
      "Input: lenght\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "restaurants O\n",
      "in O\n",
      "abc B-CITY\n",
      "##id I-CITY\n",
      "[SEP] O\n",
      "Input: restaurants in abcid\n",
      "Predicted entities: abcid\n",
      "\n",
      "[CLS] O\n",
      "allie B-CITY\n",
      "##n I-CITY\n",
      "[SEP] O\n",
      "Input: allien\n",
      "Predicted entities: allien\n",
      "\n",
      "[CLS] O\n",
      "prop O\n",
      "##osa O\n",
      "[SEP] O\n",
      "Input: proposa\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "wat O\n",
      "##her O\n",
      "[SEP] O\n",
      "Input: wather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "orlando B-CITY\n",
      "real I-ORG\n",
      "##ty I-ORG\n",
      "group I-ORG\n",
      "[SEP] O\n",
      "Input: orlando realty group\n",
      "Predicted entities: orlando realty group\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your text list\n",
    "text_list = [\n",
    "    'New York', 'Los Angeles', 'Chicago', 'Philadelphia', 'Dallas',\n",
    "    'Fort Worth', 'Houston', 'Atlanta', 'Boston', 'Manchester',\n",
    "    'Washington, D.C.', 'Hagerstown', 'San Francisco', 'Oakland',\n",
    "    'San Jose', \n",
    "    'san jose',\n",
    "    'weather in san jose',\n",
    "    'weather in Boston',\n",
    "    'Weather in Boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weather',\n",
    "    'Weather',\n",
    "    'Boston weather',\n",
    "    'Boston Weather',\n",
    "    'I love Pizzahut',\n",
    "    'I like Starbucks',\n",
    "    'sushi restaurants in Sunnyvale, CA',\n",
    "    'sushi restaurants in Sunnyvale, California',\n",
    "    'ramen in sf',\n",
    "    'sushi sf',\n",
    "    'sushi sfo',\n",
    "    'sushi sfo, CA',\n",
    "    'ramen sfo',\n",
    "    'sfo sushi',\n",
    "    'phx ramen',\n",
    "    'restaurants seattle',\n",
    "    'restaurants in seattle',\n",
    "    'restaurants near seattle',\n",
    "    'seattle restaurants',\n",
    "    'seattle wa restaurants',\n",
    "    'seattle, wa restaurants',\n",
    "    'waterloo ia hamburgers',\n",
    "    'walm',\n",
    "    'footbal',\n",
    "    'ramen ra',\n",
    "    'big city',\n",
    "    'orlando bloom', \n",
    "    'banana', \n",
    "    'lenght',\n",
    "    'restaurants in abcid',\n",
    "    'allien', 'proposa', 'wather',\n",
    "    'orlando realty group',\n",
    "    \n",
    "]\n",
    "\n",
    "model = trainer.model\n",
    "\n",
    "\n",
    "# Function to make predictions and group entities\n",
    "def predict_ner(text_list):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for text in text_list:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predictions (logits -> predicted labels)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Map the predictions to labels and tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu().numpy())\n",
    "        ner_labels = [model.config.id2label[pred] for pred in predictions]\n",
    "\n",
    "        # Group tokens back into entities\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        entities = []\n",
    "\n",
    "        for token, label in zip(tokens, ner_labels):\n",
    "            print(token, label)\n",
    "            # Ignore special tokens like [CLS], [SEP]\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            # Handle subword tokens (tokens starting with ##)\n",
    "            if token.startswith(\"##\"):\n",
    "                if current_entity:\n",
    "                    current_entity[-1] += token[2:]  # Append the subword without \"##\"\n",
    "            elif label.startswith(\"B-\") or (label.startswith(\"I-\") and label != current_label):\n",
    "                # New entity starts, append the old one\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_entity.append(token)\n",
    "                current_label = label\n",
    "            elif label.startswith(\"I-\") and label == current_label:\n",
    "                # Continue current entity\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                # Non-entity token or 'O'\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "        # Append any remaining entity\n",
    "        if current_entity:\n",
    "            entities.append(\" \".join(current_entity))\n",
    "\n",
    "        # Clean up tokens (remove subword tokens and punctuation issues, etc.)\n",
    "        clean_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.replace(\" ##\", \" \")\n",
    "            entity = entity.replace(\" .\", \".\")  # Handle punctuation\n",
    "            entity = entity.replace(\" ,\", \",\")\n",
    "            clean_entities.append(entity)\n",
    "\n",
    "        # Print the result for comparison\n",
    "        print(f\"Input: {text}\")\n",
    "        print(f\"Predicted entities: {' '.join(clean_entities)}\")\n",
    "        print()\n",
    "\n",
    "# Run predictions on the text list\n",
    "predict_ner(text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdf1c664-80e1-47a1-a33f-98e2dd509623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model (DistilBERT NER model)\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\",\n",
    "                                                             num_labels=11,\n",
    "                                                             id2label=id2label,\n",
    "                                                             label2id=label2id)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Load the LoRA-adapted model\n",
    "peft_config = PeftConfig.from_pretrained(\"results/checkpoint-148128\")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"results/checkpoint-148128\")\n",
    "\n",
    "# Merge the LoRA weights with the base model\n",
    "merged_model = lora_model.merge_and_unload()  # This merges LoRA into the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9d63ba4-2659-44b6-bf40-e33cc1516545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tmp/merged_distilbert_uncased_ner/tokenizer_config.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/special_tokens_map.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/vocab.txt',\n",
       " 'tmp/merged_distilbert_uncased_ner/added_tokens.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the merged model and tokenizer\n",
    "save_dir = \"tmp/merged_distilbert_uncased_ner\"\n",
    "merged_model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efae28df-7596-4142-9aa8-9e8e5d291f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b73fbf99-9165-4afe-a34a-a7a112427371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c889aa6-4b3d-479b-8dc7-23abf7a3164e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 265M/265M [00:06<00:00, 39.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozilla/distilbert-uncased-NER-LoRA/commit/2767ead387ed789f27076df6f51978cdf9a1b463', commit_message='Upload tokenizer', commit_description='', oid='2767ead387ed789f27076df6f51978cdf9a1b463', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozilla/distilbert-uncased-NER-LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozilla/distilbert-uncased-NER-LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the merged model\n",
    "merged_model_dir = \"tmp/merged_distilbert_uncased_ner\"\n",
    "merged_repo_id = \"Mozilla/distilbert-uncased-NER-LoRA\" \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f08c0-2488-4efb-93b2-2d89a8ff6cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
