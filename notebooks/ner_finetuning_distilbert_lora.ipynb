{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bbbb75-8c65-440e-a059-c63c2fa91996",
   "metadata": {},
   "source": [
    "purpose of this notebook is to finetune the \"dslim/bert-base-NER\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7436463-26a4-4ebb-abd1-771ee134220b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6740344d-7d09-457a-bb68-a64f2b532103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6093b56-0180-4b67-8b04-b562979979ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 20041 examples [00:00, 605204.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "full_dataset = Dataset.from_parquet(\"data/combined_ner_examples.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f06a123-bdd7-4655-ae16-92bdc655924f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 20041\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5b0e6e-e169-4e32-aeae-00c58949ff32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19041"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set_size = 1000\n",
    "val_start = len(full_dataset) - val_set_size\n",
    "val_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3d3524-3a60-4ce2-863b-08a6dadd2fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_dataset = full_dataset.select(range(val_start))  # Select training rows\n",
    "val_dataset = full_dataset.select(range(val_start, len(full_dataset)))  # Select last 1000 rows for validation\n",
    "\n",
    "# Combine them into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ffc3cd-886d-4f7f-a8c8-8f5413628646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 19041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340175ec-d224-4c3b-aaa5-16037c61fff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'].to_pandas()['tokens'].apply(len).hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2046667-d9ef-44ab-a444-dc95752478aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f20d0f6-fa8e-47d7-a60b-2e673e25685a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19041/19041 [00:05<00:00, 3334.93 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2370.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for distilbert-based NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "# Function to tokenize the input and align labels with tokens\n",
    "def tokenize_and_align_labels(example):\n",
    "    # Tokenize 'tokens' while keeping track of word boundaries\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example['tokens'], \n",
    "        is_split_into_words=True, \n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        max_length=64,\n",
    "    )\n",
    "    \n",
    "    # Get the word_ids (mapping from tokens to original words)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    aligned_labels = []\n",
    "\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)  # Special tokens ([CLS], [SEP], etc.)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(example['ner_tags'][word_idx])  # Assign the label to the first token of each word\n",
    "        else:\n",
    "            aligned_labels.append(-100)  # Subword tokens get label -100\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the function to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106935d0-9093-4ff9-8e0f-afe4aa2d792c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 19041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331d6dca-55d0-473e-94ca-90a152a9e9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 19041,\n",
       " 'tokens': ['I',\n",
       "  'am',\n",
       "  'planning',\n",
       "  'a',\n",
       "  'trip',\n",
       "  'to',\n",
       "  'alexandria,',\n",
       "  'la',\n",
       "  'next',\n",
       "  'month.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 5, 6, 0, 0],\n",
       " 'input_ids': [101,\n",
       "  146,\n",
       "  1821,\n",
       "  3693,\n",
       "  170,\n",
       "  3868,\n",
       "  1106,\n",
       "  23280,\n",
       "  20192,\n",
       "  3276,\n",
       "  3464,\n",
       "  117,\n",
       "  2495,\n",
       "  1397,\n",
       "  2370,\n",
       "  119,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1221fd4e-3f16-4ed4-9d9f-c359604545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions_and_labels(predictions, references):\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    cmp_count = 0\n",
    "\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        # Only keep labels that are not -100\n",
    "        true_labels_example = [label for label in reference if label != -100]\n",
    "        \n",
    "        # Align predictions: Remove predictions for which the corresponding reference label is -100\n",
    "        true_predictions_example = [pred for pred, ref in zip(prediction, reference) if ref != -100]\n",
    "\n",
    "        # Ensure the length of predictions and labels matches\n",
    "        if len(true_predictions_example) == len(true_labels_example):\n",
    "            true_labels.append(true_labels_example)\n",
    "            true_predictions.append(true_predictions_example)\n",
    "            cmp_count += 1\n",
    "        else:\n",
    "            # Log or handle the error (example-level mismatch)\n",
    "            # print(f\"Skipping example due to mismatch: predictions ({len(true_predictions_example)}), labels ({len(true_labels_example)})\")\n",
    "            continue  # Skip this example\n",
    "\n",
    "    # Flatten the lists (convert from list of lists to a single list)\n",
    "    true_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
    "    true_labels = [label for sublist in true_labels for label in sublist]\n",
    "    print(f\"cmp_count = {cmp_count} out of {len(predictions)}\")\n",
    "\n",
    "    return true_predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f62dee2-077d-451f-af48-60eaf50e5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Post-process the predictions and labels to remove -100 values\n",
    "    true_predictions, true_labels = postprocess_predictions_and_labels(predictions, labels)\n",
    "\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7acd16-763a-4055-b492-3007b5057da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\", num_labels=9)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,  # Task type is token classification (NER)\n",
    "    r=8,  # Low-rank dimension (you can experiment with this)\n",
    "    lora_alpha=32,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    target_modules=['q_lin']  # LoRA is applied to query layer\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c185799-1fd7-4319-a2a5-89ba1ff52df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14292' max='14292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14292/14292 15:16, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.191208</td>\n",
       "      <td>0.139165</td>\n",
       "      <td>0.750614</td>\n",
       "      <td>0.139165</td>\n",
       "      <td>0.233110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.106333</td>\n",
       "      <td>0.180915</td>\n",
       "      <td>0.761024</td>\n",
       "      <td>0.180915</td>\n",
       "      <td>0.291246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.057788</td>\n",
       "      <td>0.206759</td>\n",
       "      <td>0.756520</td>\n",
       "      <td>0.206759</td>\n",
       "      <td>0.324062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.039453</td>\n",
       "      <td>0.187447</td>\n",
       "      <td>0.757323</td>\n",
       "      <td>0.187447</td>\n",
       "      <td>0.299817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.030271</td>\n",
       "      <td>0.179494</td>\n",
       "      <td>0.764539</td>\n",
       "      <td>0.179494</td>\n",
       "      <td>0.290127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.024663</td>\n",
       "      <td>0.168418</td>\n",
       "      <td>0.761753</td>\n",
       "      <td>0.168418</td>\n",
       "      <td>0.275358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>0.161886</td>\n",
       "      <td>0.752585</td>\n",
       "      <td>0.161886</td>\n",
       "      <td>0.265950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.018402</td>\n",
       "      <td>0.161034</td>\n",
       "      <td>0.754711</td>\n",
       "      <td>0.161034</td>\n",
       "      <td>0.265074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>0.163590</td>\n",
       "      <td>0.756431</td>\n",
       "      <td>0.163590</td>\n",
       "      <td>0.268530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.015180</td>\n",
       "      <td>0.165578</td>\n",
       "      <td>0.756360</td>\n",
       "      <td>0.165578</td>\n",
       "      <td>0.271265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.014629</td>\n",
       "      <td>0.166998</td>\n",
       "      <td>0.754061</td>\n",
       "      <td>0.166998</td>\n",
       "      <td>0.273041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.167282</td>\n",
       "      <td>0.754268</td>\n",
       "      <td>0.167282</td>\n",
       "      <td>0.273438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 591 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14292, training_loss=0.030183565106139676, metrics={'train_runtime': 916.7135, 'train_samples_per_second': 249.251, 'train_steps_per_second': 15.59, 'total_flos': 3739197044081664.0, 'train_loss': 0.030183565106139676, 'epoch': 12.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Output directory\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate at the end of every epoch\n",
    "    learning_rate=2e-5,               # Learning rate\n",
    "    per_device_train_batch_size=16,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
    "    num_train_epochs=12,               # Number of training epochs\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    logging_dir='./logs',             # Directory for logging\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,                  # LoRA-wrapped model\n",
    "    args=training_args,                # Training arguments\n",
    "    train_dataset=tokenized_dataset['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],  # Validation dataset (if available)\n",
    "    tokenizer=tokenizer,               # Tokenizer\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82880e92-4d83-442a-9a1c-3a2386b1c942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] O\n",
      "New B-LOC\n",
      "York I-LOC\n",
      "[SEP] I-LOC\n",
      "Input: New York\n",
      "Predicted entities: New York\n",
      "\n",
      "[CLS] O\n",
      "Los B-LOC\n",
      "Angeles I-LOC\n",
      "[SEP] I-LOC\n",
      "Input: Los Angeles\n",
      "Predicted entities: Los Angeles\n",
      "\n",
      "[CLS] O\n",
      "Chicago B-LOC\n",
      "[SEP] O\n",
      "Input: Chicago\n",
      "Predicted entities: Chicago\n",
      "\n",
      "[CLS] O\n",
      "Philadelphia B-LOC\n",
      "[SEP] O\n",
      "Input: Philadelphia\n",
      "Predicted entities: Philadelphia\n",
      "\n",
      "[CLS] O\n",
      "Dallas B-LOC\n",
      "[SEP] O\n",
      "Input: Dallas\n",
      "Predicted entities: Dallas\n",
      "\n",
      "[CLS] O\n",
      "Fort B-LOC\n",
      "Worth I-LOC\n",
      "[SEP] I-LOC\n",
      "Input: Fort Worth\n",
      "Predicted entities: Fort Worth\n",
      "\n",
      "[CLS] O\n",
      "Houston B-LOC\n",
      "[SEP] O\n",
      "Input: Houston\n",
      "Predicted entities: Houston\n",
      "\n",
      "[CLS] O\n",
      "Atlanta B-LOC\n",
      "[SEP] O\n",
      "Input: Atlanta\n",
      "Predicted entities: Atlanta\n",
      "\n",
      "[CLS] O\n",
      "Boston B-LOC\n",
      "[SEP] O\n",
      "Input: Boston\n",
      "Predicted entities: Boston\n",
      "\n",
      "[CLS] O\n",
      "Manchester B-ORG\n",
      "[SEP] I-LOC\n",
      "Input: Manchester\n",
      "Predicted entities: Manchester\n",
      "\n",
      "[CLS] O\n",
      "Washington B-LOC\n",
      ", O\n",
      "D B-LOC\n",
      ". B-LOC\n",
      "C I-LOC\n",
      ". O\n",
      "[SEP] I-LOC\n",
      "Input: Washington, D.C.\n",
      "Predicted entities: Washington D . C\n",
      "\n",
      "[CLS] O\n",
      "Ha B-LOC\n",
      "##gers B-LOC\n",
      "##town B-LOC\n",
      "[SEP] O\n",
      "Input: Hagerstown\n",
      "Predicted entities: Hagerstown\n",
      "\n",
      "[CLS] B-LOC\n",
      "San B-LOC\n",
      "Francisco I-LOC\n",
      "[SEP] I-LOC\n",
      "Input: San Francisco\n",
      "Predicted entities: San Francisco\n",
      "\n",
      "[CLS] O\n",
      "Oakland B-ORG\n",
      "[SEP] O\n",
      "Input: Oakland\n",
      "Predicted entities: Oakland\n",
      "\n",
      "[CLS] O\n",
      "San B-LOC\n",
      "Jose I-LOC\n",
      "[SEP] I-LOC\n",
      "Input: San Jose\n",
      "Predicted entities: San Jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "sa B-LOC\n",
      "##n B-LOC\n",
      "j I-LOC\n",
      "##ose I-LOC\n",
      "[SEP] O\n",
      "Input: weather in san jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "Boston B-LOC\n",
      "[SEP] O\n",
      "Input: weather in Boston\n",
      "Predicted entities: Boston\n",
      "\n",
      "[CLS] O\n",
      "Weather O\n",
      "in O\n",
      "Boston B-LOC\n",
      "[SEP] O\n",
      "Input: Weather in Boston\n",
      "Predicted entities: Boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "Boston B-LOC\n",
      "[SEP] O\n",
      "Input: weather Boston\n",
      "Predicted entities: Boston\n",
      "\n",
      "[CLS] O\n",
      "Weather O\n",
      "Boston I-ORG\n",
      "[SEP] O\n",
      "Input: Weather Boston\n",
      "Predicted entities: Boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "Weather O\n",
      "[SEP] O\n",
      "Input: Weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "Boston B-LOC\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: Boston weather\n",
      "Predicted entities: Boston\n",
      "\n",
      "[CLS] O\n",
      "Boston B-LOC\n",
      "Weather I-ORG\n",
      "[SEP] I-LOC\n",
      "Input: Boston Weather\n",
      "Predicted entities: Boston Weather\n",
      "\n",
      "[CLS] O\n",
      "I O\n",
      "love O\n",
      "Pizza B-PER\n",
      "##hu B-PER\n",
      "##t B-PER\n",
      "[SEP] O\n",
      "Input: I love Pizzahut\n",
      "Predicted entities: Pizzahut\n",
      "\n",
      "[CLS] O\n",
      "I O\n",
      "like O\n",
      "Star B-ORG\n",
      "##bu B-ORG\n",
      "##cks B-ORG\n",
      "[SEP] O\n",
      "Input: I like Starbucks\n",
      "Predicted entities: Starbucks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your text list\n",
    "text_list = [\n",
    "    'New York', 'Los Angeles', 'Chicago', 'Philadelphia', 'Dallas',\n",
    "    'Fort Worth', 'Houston', 'Atlanta', 'Boston', 'Manchester',\n",
    "    'Washington, D.C.', 'Hagerstown', 'San Francisco', 'Oakland',\n",
    "    'San Jose', \n",
    "    # 'san jose',\n",
    "    'weather in san jose',\n",
    "    'weather in Boston',\n",
    "    'Weather in Boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weather',\n",
    "    'Weather',\n",
    "    'Boston weather',\n",
    "    'Boston Weather',\n",
    "    'I love Pizzahut',\n",
    "    'I like Starbucks',\n",
    "]\n",
    "\n",
    "model = trainer.model\n",
    "\n",
    "# Function to make predictions and group entities\n",
    "def predict_ner(text_list):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for text in text_list:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predictions (logits -> predicted labels)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Map the predictions to labels and tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu().numpy())\n",
    "        ner_labels = [model.config.id2label[pred] for pred in predictions]\n",
    "\n",
    "        # Group tokens back into entities\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        entities = []\n",
    "\n",
    "        for token, label in zip(tokens, ner_labels):\n",
    "            print(token, label)\n",
    "            # Ignore special tokens like [CLS], [SEP]\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            # Handle subword tokens (tokens starting with ##)\n",
    "            if token.startswith(\"##\"):\n",
    "                current_entity[-1] += token[2:]  # Append the subword without \"##\"\n",
    "            elif label.startswith(\"B-\") or (label.startswith(\"I-\") and label != current_label):\n",
    "                # New entity starts, append the old one\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_entity.append(token)\n",
    "                current_label = label\n",
    "            elif label.startswith(\"I-\") and label == current_label:\n",
    "                # Continue current entity\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                # Non-entity token or 'O'\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "        # Append any remaining entity\n",
    "        if current_entity:\n",
    "            entities.append(\" \".join(current_entity))\n",
    "\n",
    "        # Clean up tokens (remove subword tokens and punctuation issues, etc.)\n",
    "        clean_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.replace(\" ##\", \"\")\n",
    "            entity = entity.replace(\" .\", \".\")  # Handle punctuation\n",
    "            entity = entity.replace(\" ,\", \",\")\n",
    "            clean_entities.append(entity)\n",
    "\n",
    "        # Print the result for comparison\n",
    "        print(f\"Input: {text}\")\n",
    "        print(f\"Predicted entities: {' '.join(clean_entities)}\")\n",
    "        print()\n",
    "\n",
    "# Run predictions on the text list\n",
    "predict_ner(text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdf1c664-80e1-47a1-a33f-98e2dd509623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model (DistilBERT NER model)\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "# Load the LoRA-adapted model\n",
    "peft_config = PeftConfig.from_pretrained(\"results/checkpoint-14000\")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"results/checkpoint-14000\")\n",
    "\n",
    "# Merge the LoRA weights with the base model\n",
    "merged_model = lora_model.merge_and_unload()  # This merges LoRA into the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d63ba4-2659-44b6-bf40-e33cc1516545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tmp/merged_distilbert_ner/tokenizer_config.json',\n",
       " 'tmp/merged_distilbert_ner/special_tokens_map.json',\n",
       " 'tmp/merged_distilbert_ner/vocab.txt',\n",
       " 'tmp/merged_distilbert_ner/added_tokens.json',\n",
       " 'tmp/merged_distilbert_ner/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the merged model and tokenizer\n",
    "save_dir = \"tmp/merged_distilbert_ner\"\n",
    "merged_model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efae28df-7596-4142-9aa8-9e8e5d291f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b73fbf99-9165-4afe-a34a-a7a112427371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c889aa6-4b3d-479b-8dc7-23abf7a3164e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261M/261M [00:07<00:00, 35.3MB/s] \n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozilla/distilbert-NER-LoRA/commit/105fc9c5865a957b6fa91eeb26d6cb602edb01cb', commit_message='Upload tokenizer', commit_description='', oid='105fc9c5865a957b6fa91eeb26d6cb602edb01cb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozilla/distilbert-NER-LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozilla/distilbert-NER-LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the merged model\n",
    "merged_model_dir = \"tmp/merged_distilbert_ner\"\n",
    "merged_repo_id = \"Mozilla/distilbert-NER-LoRA\" \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f08c0-2488-4efb-93b2-2d89a8ff6cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
