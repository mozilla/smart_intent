{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83df52ac-a343-49ec-83bc-16b8c91b9dd0",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to use LORA (aka Low Rank Adaptation method) and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86420020-5a69-4a3b-a6be-6ce274be8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -q datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54679b61-b9f8-4a5c-a846-0f11a1946d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip uninstall -y pyarrow datasets ibis-framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bd4be8-9789-447d-880a-264763f117fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyarrow>=15.0.0 datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05e4a8e-313a-48e5-9cb6-16ec108133bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip show pyarrow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a5ba79-b2c6-4ab7-b0dd-940eeab27cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyarrow>=15.0.0 datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c683dd-30bd-448a-ada9-bc4562b61663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                         AutoConfig,\n",
    "                         AutoModelForSequenceClassification,\n",
    "                         DataCollatorWithPadding,\n",
    "                         TrainingArguments,\n",
    "                         Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32ae2ac-7f08-4fc4-baa7-10a7ccbcf800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e512cf-4776-4278-9612-ec5c8be44f80",
   "metadata": {},
   "source": [
    "#### Base Model (mobileBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1dc1475-55e2-42ca-a881-8f33e475f41c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'google/mobilebert-uncased'\n",
    "id2label = {0: 'information_intent',\n",
    "            1: 'yelp_intent',\n",
    "            2: 'navigation_intent',\n",
    "            3: 'travel_intent',\n",
    "            4: 'purchase_intent',\n",
    "            5: 'weather_intent',\n",
    "            6: 'translation_intent',\n",
    "            7: 'unknown'}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "\n",
    "# generate classification model from model chckpoints\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=8,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a893d3-2bf9-45cb-b678-436748405b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeaa99b-10c2-44ed-8d21-08ed2b3990bd",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81eb7dac-565f-4247-b42d-fe2d7675ec60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148123\n",
      "target\n",
      "information_intent    114381\n",
      "yelp_intent            18178\n",
      "weather_intent         13196\n",
      "navigation_intent       1350\n",
      "purchase_intent          519\n",
      "travel_intent            322\n",
      "translation_intent       162\n",
      "unknown                   15\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medication used to treat cold sores</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is netflix canceling</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>definition of ethnographic</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what are housing rates</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transmission fluid flush how often</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what is trade</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what does severance package mean</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>how to purify water in the wilderness</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what are the criteria for</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what is property tax in san diego california</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sequence              target\n",
       "0           medication used to treat cold sores         yelp_intent\n",
       "1                     what is netflix canceling  information_intent\n",
       "2                    definition of ethnographic  information_intent\n",
       "3                        what are housing rates  information_intent\n",
       "4            transmission fluid flush how often  information_intent\n",
       "5                                 what is trade  information_intent\n",
       "6              what does severance package mean  information_intent\n",
       "7         how to purify water in the wilderness  information_intent\n",
       "8                     what are the criteria for  information_intent\n",
       "9  what is property tax in san diego california  information_intent"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/marco_train_v2.csv\")\n",
    "print(len(df))\n",
    "print(df['target'].value_counts())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb24484d-7380-49b6-b67f-5b645e46cf49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    148123.000000\n",
      "mean         28.956239\n",
      "std          10.173896\n",
      "min           6.000000\n",
      "10%          17.000000\n",
      "20%          21.000000\n",
      "25%          22.000000\n",
      "30%          23.000000\n",
      "40%          26.000000\n",
      "50%          28.000000\n",
      "60%          30.000000\n",
      "70%          33.000000\n",
      "75%          34.000000\n",
      "80%          36.000000\n",
      "90%          41.000000\n",
      "95%          45.000000\n",
      "98%          53.000000\n",
      "99%          59.000000\n",
      "99.5%        68.000000\n",
      "99.8%        84.000000\n",
      "99.9%        97.000000\n",
      "max         193.000000\n",
      "Name: sequence, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df['sequence'].apply(len).describe(percentiles=[.1, .2, .25, .3, .4, .5, .6, .7, .75, .8, .9, .95, .98, .99, .995, .998, .999]))\n",
    "df['sequence'].apply(len).hist(bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "082fb996-037b-4353-8739-7047d6142f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>who is the master chief petty officer of the coast guard reserve force</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>what are appropriate means for leaving evidence of presence? (select</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>what type of ideology has gained popularity throughout latin america?</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>if a president is accused of breaking the twenty-second amendment, that means the president is t...</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>​a legal maximum on the price at which a good can be sold is called a price</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127601</th>\n",
       "      <td>which president pushed through congress the civil rights act of 1964 brainly</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127656</th>\n",
       "      <td>which of the following basic allowance for housing rate is for soldiers living in barracks</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127936</th>\n",
       "      <td>for a typical person who leads a sedentary lifestyle, a typical maximum oxygen intake level woul...</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128054</th>\n",
       "      <td>which bonds are created during the formation of the primary structure of a protein?</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143607</th>\n",
       "      <td>What is the phone number for a restaurant in Wichita Falls &amp; Lawton</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>970 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   sequence  \\\n",
       "54                                   who is the master chief petty officer of the coast guard reserve force   \n",
       "71                                     what are appropriate means for leaving evidence of presence? (select   \n",
       "98                                    what type of ideology has gained popularity throughout latin america?   \n",
       "147     if a president is accused of breaking the twenty-second amendment, that means the president is t...   \n",
       "173                             ​a legal maximum on the price at which a good can be sold is called a price   \n",
       "...                                                                                                     ...   \n",
       "127601                         which president pushed through congress the civil rights act of 1964 brainly   \n",
       "127656           which of the following basic allowance for housing rate is for soldiers living in barracks   \n",
       "127936  for a typical person who leads a sedentary lifestyle, a typical maximum oxygen intake level woul...   \n",
       "128054                  which bonds are created during the formation of the primary structure of a protein?   \n",
       "143607                                  What is the phone number for a restaurant in Wichita Falls & Lawton   \n",
       "\n",
       "                    target  \n",
       "54      information_intent  \n",
       "71      information_intent  \n",
       "98      information_intent  \n",
       "147     information_intent  \n",
       "173     information_intent  \n",
       "...                    ...  \n",
       "127601  information_intent  \n",
       "127656  information_intent  \n",
       "127936  information_intent  \n",
       "128054  information_intent  \n",
       "143607         yelp_intent  \n",
       "\n",
       "[970 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.loc[df['sequence'].apply(lambda text: len(text) > 64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e0d5f3-b927-477e-ac5b-5e05fce4835b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "token_lengths = []\n",
    "for sequence in df['sequence'].values:\n",
    "    tokens = tokenizer(sequence, truncation=False)['input_ids']  # Get tokenized input IDs\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "temp_df = pd.DataFrame({'sequence': df['sequence'].values, 'token_length': token_lengths})\n",
    "\n",
    "# Display token lengths\n",
    "print(temp_df)\n",
    "\n",
    "# Optional: Analyze token lengths for deciding the best max_length\n",
    "print(f\"Max token length: {temp_df['token_length'].max()}\")\n",
    "print(f\"Average token length: {temp_df['token_length'].mean()}\")\n",
    "print(f\"90th percentile token length: {temp_df['token_length'].quantile(0.9)}\")\n",
    "print(f\"95th percentile token length: {temp_df['token_length'].quantile(0.95)}\")\n",
    "print(f\"98th percentile token length: {temp_df['token_length'].quantile(0.98)}\")\n",
    "print(f\"99th percentile token length: {temp_df['token_length'].quantile(0.99)}\")\n",
    "print(f\"99.5th percentile token length: {temp_df['token_length'].quantile(0.995)}\")\n",
    "print(f\"99.9th percentile token length: {temp_df['token_length'].quantile(0.999)}\")\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e6c934-fdc6-4d28-9c87-69eec74e5beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    22876\n",
      "1    18178\n",
      "5    13196\n",
      "2     1350\n",
      "4      519\n",
      "3      322\n",
      "6      162\n",
      "7       15\n",
      "Name: count, dtype: int64\n",
      "Size of sampled_df = 56618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_6462/3487378760.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('target', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marijuana stocks price</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where is lugano switzerland</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how to read an architect scale</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is tuition at michigan state university</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how to monitor gpu temperature</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sequence              target  label\n",
       "0                        marijuana stocks price  information_intent      0\n",
       "1                   where is lugano switzerland  information_intent      0\n",
       "2                how to read an architect scale  information_intent      0\n",
       "3  what is tuition at michigan state university  information_intent      0\n",
       "4                how to monitor gpu temperature  information_intent      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only a sample from the actual data\n",
    "\n",
    "sampling_percentages = {\n",
    "    'information_intent': 0.2,  # 20% sampling for information_intent\n",
    "    'yelp_intent': 1.0,          # 100% sampling for yelp_intent\n",
    "    'weather_intent': 1.0,       # 100% sampling for weather_intent\n",
    "    'navigation_intent': 1.0,    # 100% sampling for navigation_intent\n",
    "    'purchase_intent': 1.0,      # 100% sampling for purchase_intent\n",
    "    'translation_intent': 1.0,   # 100% sampling for translation_intent\n",
    "    'travel_intent': 1.0,        # 100% sampling for travel_intent\n",
    "    'unknown': 1.0               # 100% sampling for unknown\n",
    "}\n",
    "\n",
    "# Sample from each target group based on the defined percentages\n",
    "sampled_df = df.groupby('target', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=sampling_percentages.get(x.name, 1.0))\n",
    ").reset_index(drop=True)\n",
    "\n",
    "sampled_df['label'] = sampled_df['target'].map(label2id)\n",
    "# sampled_df = sampled_df.rename(columns={'target': 'label'})\n",
    "\n",
    "print(sampled_df['label'].value_counts())\n",
    "print(f\"Size of sampled_df = {len(sampled_df)}\")\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dadb9a2-6184-4e50-bf21-8063e31edbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sequence', 'target', 'label'],\n",
      "        num_rows: 53787\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sequence', 'target', 'label'],\n",
      "        num_rows: 2831\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the DataFrame into train and validation sets\n",
    "train_df, val_df = train_test_split(sampled_df, test_size=0.05, random_state=42, stratify=sampled_df['label'])\n",
    "\n",
    "# Step 2: Convert Pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "# Step 3: Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "# Step 4: Verify the structure of DatasetDict\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0613b1e7-e4ec-4b18-91a2-2648c42a91b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    21732\n",
       "1    17269\n",
       "5    12536\n",
       "2     1283\n",
       "4      493\n",
       "3      306\n",
       "6      154\n",
       "7       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d476d6-4aa2-4180-889b-49f8e5251219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1144\n",
       "1     909\n",
       "5     660\n",
       "2      67\n",
       "4      26\n",
       "3      16\n",
       "6       8\n",
       "7       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae60f7-3f28-4d95-bee7-28d05b68566f",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17cb2d99-4f2f-4caa-8f1f-747fbc9c9082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"sequence\"]\n",
    "\n",
    "    # tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"right\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,  # Pad the sequences to the longest in the batch\n",
    "        max_length=64\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d52d8863-7705-40d2-a6b4-26b1865f5c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 53787/53787 [00:02<00:00, 23380.31 examples/s]\n",
      "Map: 100%|██████████| 2831/2831 [00:00<00:00, 25212.06 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'target', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 53787\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'target', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2831\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "# tokenized_dataset = tokenized_dataset.map(fix_labels)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56166c5b-958c-4921-961a-7e42c1ef37d7",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d481d862-dfe6-4635-9d0c-7c827a2155f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f08d8464-0714-4021-a266-9ceb64f36f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "floor repair cost -> information_intent\n",
      "denture fix -> information_intent\n",
      "who is the us president -> information_intent\n",
      "italian food -> information_intent\n",
      "sandwiches in seattle -> information_intent\n",
      "seattle weather -> information_intent\n",
      "weather seattle -> information_intent\n",
      "boston wether -> information_intent\n",
      "Boston wether -> information_intent\n",
      "weather boston -> information_intent\n",
      "weather Boston -> information_intent\n",
      "Weather Boston -> information_intent\n",
      "weathr boston -> information_intent\n",
      "seattle weathr -> information_intent\n"
     ]
    }
   ],
   "source": [
    "### Evaluate untrained model\n",
    "\n",
    "text_list = [\n",
    "    'floor repair cost',\n",
    "    'denture fix',\n",
    "    'who is the us president',\n",
    "    'italian food',\n",
    "    'sandwiches in seattle',\n",
    "    'seattle weather',\n",
    "    'weather seattle',\n",
    "    'boston wether',\n",
    "    'Boston wether',\n",
    "    'weather boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weathr boston',\n",
    "    'seattle weathr',\n",
    "]\n",
    "\n",
    "sample_labels = [\n",
    "    label2id[\"yelp_intent\"],\n",
    "    label2id[\"yelp_intent\"],\n",
    "    label2id[\"information_intent\"],\n",
    "    label2id[\"yelp_intent\"],\n",
    "    label2id[\"yelp_intent\"]\n",
    "]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "predictions = []\n",
    "logits_list = []\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    logits = model(inputs).logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    predictions.append(prediction)\n",
    "    print(text + \" -> \" + id2label[prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfb5ad-391d-42da-bcf5-d62e39642225",
   "metadata": {},
   "source": [
    "#### Model finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cece8cd8-b017-4f80-9d9a-713041270fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 28,680 || all params: 24,614,672 || trainable%: 0.1165\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                         r=4, # intrinsic rank of trainable weight matrix\n",
    "                         lora_alpha=32, # similar to learning_rate\n",
    "                         lora_dropout=0.01, # probability of dropout nodes\n",
    "                         target_modules=['attention.self.query']) # LoRA is applied to query layer\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93c9ac1c-cd57-4173-b208-78831d9c4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccdcde-6b2c-499d-a185-d04dbd1b6ba7",
   "metadata": {},
   "source": [
    "#### Define hyper parameters and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84297802-ae8a-4fef-882a-7d7145de72bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=model_checkpoint + \"-lora-intent-classification-v3\",\n",
    "    output_dir=\"mobilebert-uncased\" + \"-lora-intent-classification-v3\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "615cdb83-edc7-409a-a929-6c370c8aa9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, # Hyperparamaters\n",
    "    train_dataset=tokenized_dataset[\"train\"], # training data\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # validation data\n",
    "    tokenizer=tokenizer, # tokenizer\n",
    "    data_collator=data_collator, # dynamic sequence padding\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9fde457-1a0d-4552-b6fa-1c0642349972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33620' max='33620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33620/33620 58:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>206.533300</td>\n",
       "      <td>0.685373</td>\n",
       "      <td>0.937831</td>\n",
       "      <td>0.937266</td>\n",
       "      <td>0.937831</td>\n",
       "      <td>0.936803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1014.233100</td>\n",
       "      <td>1.700965</td>\n",
       "      <td>0.942423</td>\n",
       "      <td>0.942275</td>\n",
       "      <td>0.942423</td>\n",
       "      <td>0.941208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>964.142300</td>\n",
       "      <td>6.571901</td>\n",
       "      <td>0.948428</td>\n",
       "      <td>0.948305</td>\n",
       "      <td>0.948428</td>\n",
       "      <td>0.947598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>767.387600</td>\n",
       "      <td>21.453617</td>\n",
       "      <td>0.953373</td>\n",
       "      <td>0.953291</td>\n",
       "      <td>0.953373</td>\n",
       "      <td>0.952601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>518.152000</td>\n",
       "      <td>0.149336</td>\n",
       "      <td>0.961851</td>\n",
       "      <td>0.961294</td>\n",
       "      <td>0.961851</td>\n",
       "      <td>0.961495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>306.871800</td>\n",
       "      <td>2.405443</td>\n",
       "      <td>0.961498</td>\n",
       "      <td>0.960648</td>\n",
       "      <td>0.961498</td>\n",
       "      <td>0.960416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>352.161000</td>\n",
       "      <td>14.204342</td>\n",
       "      <td>0.961851</td>\n",
       "      <td>0.961684</td>\n",
       "      <td>0.961851</td>\n",
       "      <td>0.961635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>41.528300</td>\n",
       "      <td>2.416231</td>\n",
       "      <td>0.970329</td>\n",
       "      <td>0.970017</td>\n",
       "      <td>0.970329</td>\n",
       "      <td>0.969768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>36.199600</td>\n",
       "      <td>6.597254</td>\n",
       "      <td>0.968562</td>\n",
       "      <td>0.968283</td>\n",
       "      <td>0.968562</td>\n",
       "      <td>0.968328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.333000</td>\n",
       "      <td>0.107496</td>\n",
       "      <td>0.972448</td>\n",
       "      <td>0.972365</td>\n",
       "      <td>0.972448</td>\n",
       "      <td>0.972356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=33620, training_loss=2304.9549859532, metrics={'train_runtime': 3502.7715, 'train_samples_per_second': 153.556, 'train_steps_per_second': 9.598, 'total_flos': 2644552736296416.0, 'train_loss': 2304.9549859532, 'epoch': 10.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1da3c5f1-ef7f-4ca1-9792-4879b621f267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15d874c9-e652-4c17-8a54-11d94710debb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floor repair cost -> yelp_intent\n",
      "denture fix -> yelp_intent\n",
      "who is the us president -> information_intent\n",
      "italian food -> yelp_intent\n",
      "sandwiches in seattle -> yelp_intent\n",
      "seattle weather -> weather_intent\n",
      "weather seattle -> weather_intent\n",
      "boston wether -> weather_intent\n",
      "Boston wether -> weather_intent\n",
      "weather boston -> weather_intent\n",
      "weather Boston -> weather_intent\n",
      "Weather Boston -> weather_intent\n",
      "weathr boston -> weather_intent\n",
      "seattle weathr -> weather_intent\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in text_list:\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "        logits = trainer.model(inputs).logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        print(text + \" -> \" + id2label[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66aae059-b207-43d5-8a78-a76901550c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.4M\n",
      "-rw-r--r-- 1 jupyter jupyter 5.0K Oct 10 12:36 README.md\n",
      "-rw-r--r-- 1 jupyter jupyter  679 Oct 10 12:36 adapter_config.json\n",
      "-rw-r--r-- 1 jupyter jupyter 119K Oct 10 12:36 adapter_model.safetensors\n",
      "-rw-r--r-- 1 jupyter jupyter 266K Oct 10 12:36 optimizer.pt\n",
      "-rw-r--r-- 1 jupyter jupyter  14K Oct 10 12:36 rng_state.pth\n",
      "-rw-r--r-- 1 jupyter jupyter 1.1K Oct 10 12:36 scheduler.pt\n",
      "-rw-r--r-- 1 jupyter jupyter  125 Oct 10 12:36 special_tokens_map.json\n",
      "-rw-r--r-- 1 jupyter jupyter 695K Oct 10 12:36 tokenizer.json\n",
      "-rw-r--r-- 1 jupyter jupyter 1.3K Oct 10 12:36 tokenizer_config.json\n",
      "-rw-r--r-- 1 jupyter jupyter  16K Oct 10 12:36 trainer_state.json\n",
      "-rw-r--r-- 1 jupyter jupyter 5.2K Oct 10 12:36 training_args.bin\n",
      "-rw-r--r-- 1 jupyter jupyter 227K Oct 10 12:36 vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -lh mobilebert-uncased-lora-intent-classification-v3/checkpoint-33620"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08157f-ff0c-46a3-aa0d-f04493a71d36",
   "metadata": {},
   "source": [
    "#### Load the LoRA model from checkpoint after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "041cee58-787b-430c-a9c4-c4aadcf87a58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37367.0977, 37333.8594, 37001.4570, 29453.9238, 35999.8438, 37165.7148,\n",
      "         34981.0977, 17471.5176]], grad_fn=<AddmmBackward0>)\n",
      "0 information_intent\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'information_intent',\n",
    "            1: 'yelp_intent',\n",
    "            2: 'navigation_intent',\n",
    "            3: 'travel_intent',\n",
    "            4: 'purchase_intent',\n",
    "            5: 'weather_intent',\n",
    "            6: 'translation_intent',\n",
    "            7: 'unknown'}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "\n",
    "output_dir = \"mobilebert-uncased-lora-intent-classification-v3/checkpoint-33620\"\n",
    "\n",
    "# Load the tokenizer (from the output directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Load the base model from the original checkpoint (base pre-trained model)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Load the LoRA configuration and model\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Step 3: Save the combined model to a directory\n",
    "save_directory = \"tmp/mobilebert_lora_combined_model/\"\n",
    "lora_model.save_pretrained(save_directory)  # Save base model + LoRA weights\n",
    "\n",
    "# Now the `lora_model` contains both the base model and the LoRA weights.\n",
    "lora_model.eval()\n",
    "\n",
    "# Example inference\n",
    "inputs = tokenizer([\"looking for home cleaning \"], return_tensors=\"pt\")\n",
    "outputs = lora_model(**inputs)\n",
    "logits = outputs.logits\n",
    "print(logits)\n",
    "\n",
    "\n",
    "prediction = torch.argmax(logits, dim=1).item()\n",
    "print(prediction, id2label[prediction])\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "rounded_probabilities = torch.round(probabilities)\n",
    "print(rounded_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5f259a5-9032-4614-b0fc-b6c685ea250e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tmp/mobilebert_lora_combined_model/tokenizer_config.json',\n",
       " 'tmp/mobilebert_lora_combined_model/special_tokens_map.json',\n",
       " 'tmp/mobilebert_lora_combined_model/vocab.txt',\n",
       " 'tmp/mobilebert_lora_combined_model/added_tokens.json',\n",
       " 'tmp/mobilebert_lora_combined_model/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Step 1: Load the base model (DistilBERT)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Step 2: Load the LoRA adapter weights\n",
    "output_dir = \"mobilebert-uncased-lora-intent-classification-v3/checkpoint-33620\"\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Step 3: Merge LoRA weights into the base model\n",
    "# After this, the model will have both base and LoRA weights applied\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Step 4: Save the full model (base model + LoRA weights)\n",
    "save_directory = \"tmp/mobilebert_lora_combined_model/\"\n",
    "merged_model.save_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)  # Load the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)  # Save the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7eb33ea5-05c4-4417-8c86-05782471d680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75982821-faaf-45ba-9936-26f1c275f78f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model_dir = \"tmp/mobilebert_lora_combined_model\"\n",
    "merged_repo_id = \"Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier\"  \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1bcba70-77f1-493e-8c35-e135ec2f9893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ce05e-44be-4fc1-b8bb-ada9f1415386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
