{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83df52ac-a343-49ec-83bc-16b8c91b9dd0",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to use LORA (aka Low Rank Adaptation method) and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86420020-5a69-4a3b-a6be-6ce274be8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -q datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54679b61-b9f8-4a5c-a846-0f11a1946d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip uninstall -y pyarrow datasets ibis-framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bd4be8-9789-447d-880a-264763f117fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyarrow>=15.0.0 datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05e4a8e-313a-48e5-9cb6-16ec108133bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip show pyarrow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a5ba79-b2c6-4ab7-b0dd-940eeab27cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyarrow>=15.0.0 datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c683dd-30bd-448a-ada9-bc4562b61663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                         AutoConfig,\n",
    "                         AutoModelForSequenceClassification,\n",
    "                         DataCollatorWithPadding,\n",
    "                         TrainingArguments,\n",
    "                         Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32ae2ac-7f08-4fc4-baa7-10a7ccbcf800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e512cf-4776-4278-9612-ec5c8be44f80",
   "metadata": {},
   "source": [
    "#### Base Model (mobileBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1dc1475-55e2-42ca-a881-8f33e475f41c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'google/mobilebert-uncased'\n",
    "id2label = {0: 'information_intent',\n",
    "            1: 'yelp_intent',\n",
    "            2: 'navigation_intent',\n",
    "            3: 'travel_intent',\n",
    "            4: 'purchase_intent',\n",
    "            5: 'weather_intent',\n",
    "            6: 'translation_intent',\n",
    "            7: 'unknown'}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "\n",
    "# generate classification model from model chckpoints\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=8,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeaa99b-10c2-44ed-8d21-08ed2b3990bd",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81eb7dac-565f-4247-b42d-fe2d7675ec60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196252\n",
      "target\n",
      "information_intent    120902\n",
      "yelp_intent            28290\n",
      "navigation_intent      14332\n",
      "weather_intent         13750\n",
      "purchase_intent         7893\n",
      "travel_intent           6686\n",
      "translation_intent      2294\n",
      "unknown                 2105\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Breaking Bad</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is japanese knotweed</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what framing square used for</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diseases that are caused by bacteria</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how to register on expedia.com</td>\n",
       "      <td>navigation_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the definition of hypertrichosis</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>how does a person become a united states citizen?</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the waether in Cape Girardeau</td>\n",
       "      <td>weather_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>characteristics of pacemaker cells</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Best Tatcha face mask products to buy this year</td>\n",
       "      <td>purchase_intent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence              target\n",
       "0                                       Breaking Bad  information_intent\n",
       "1                          what is japanese knotweed  information_intent\n",
       "2                       what framing square used for  information_intent\n",
       "3               diseases that are caused by bacteria  information_intent\n",
       "4                     how to register on expedia.com   navigation_intent\n",
       "5                   the definition of hypertrichosis  information_intent\n",
       "6  how does a person become a united states citizen?  information_intent\n",
       "7              What is the waether in Cape Girardeau      weather_intent\n",
       "8                 characteristics of pacemaker cells  information_intent\n",
       "9    Best Tatcha face mask products to buy this year     purchase_intent"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/marco_train_v6.csv\")\n",
    "print(len(df))\n",
    "print(df['target'].value_counts())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb24484d-7380-49b6-b67f-5b645e46cf49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    196252.000000\n",
      "mean         28.918737\n",
      "std          11.115094\n",
      "min           3.000000\n",
      "10%          16.000000\n",
      "20%          20.000000\n",
      "25%          21.000000\n",
      "30%          23.000000\n",
      "40%          25.000000\n",
      "50%          28.000000\n",
      "60%          31.000000\n",
      "70%          34.000000\n",
      "75%          35.000000\n",
      "80%          37.000000\n",
      "90%          43.000000\n",
      "95%          48.000000\n",
      "98%          54.000000\n",
      "99%          60.000000\n",
      "99.5%        65.000000\n",
      "99.8%        79.000000\n",
      "99.9%        92.000000\n",
      "max         193.000000\n",
      "Name: sequence, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df['sequence'].apply(len).describe(percentiles=[.1, .2, .25, .3, .4, .5, .6, .7, .75, .8, .9, .95, .98, .99, .995, .998, .999]))\n",
    "df['sequence'].apply(len).hist(bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082fb996-037b-4353-8739-7047d6142f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>how to determine if a piecewise multivariable function is continuous</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>which amendment to the constitution is known as the prohibition amendment? quizlet</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>the _______ is the layer of the atmosphere in which weather occurs. a. exosphere b. mesosphere c...</td>\n",
       "      <td>weather_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>does your ira contribution have to be deposited or mailed by deadline</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>Compare prices for relocation services in Los Angeles, California</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195475</th>\n",
       "      <td>which pr option involves members of government to affect recovery</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195519</th>\n",
       "      <td>​a legal maximum on the price at which a good can be sold is called a price</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195543</th>\n",
       "      <td>which of the following organelles are common to both prokaryotic and eukaryotic cells?</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195907</th>\n",
       "      <td>what was the most important revolution of the industrial revolution</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195972</th>\n",
       "      <td>which of the following statements is consistent with the second law of thermodynamics?</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1072 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   sequence  \\\n",
       "83                                     how to determine if a piecewise multivariable function is continuous   \n",
       "125                      which amendment to the constitution is known as the prohibition amendment? quizlet   \n",
       "182     the _______ is the layer of the atmosphere in which weather occurs. a. exosphere b. mesosphere c...   \n",
       "682                                   does your ira contribution have to be deposited or mailed by deadline   \n",
       "1249                                      Compare prices for relocation services in Los Angeles, California   \n",
       "...                                                                                                     ...   \n",
       "195475                                    which pr option involves members of government to affect recovery   \n",
       "195519                          ​a legal maximum on the price at which a good can be sold is called a price   \n",
       "195543               which of the following organelles are common to both prokaryotic and eukaryotic cells?   \n",
       "195907                                  what was the most important revolution of the industrial revolution   \n",
       "195972               which of the following statements is consistent with the second law of thermodynamics?   \n",
       "\n",
       "                    target  \n",
       "83      information_intent  \n",
       "125     information_intent  \n",
       "182         weather_intent  \n",
       "682     information_intent  \n",
       "1249           yelp_intent  \n",
       "...                    ...  \n",
       "195475  information_intent  \n",
       "195519  information_intent  \n",
       "195543  information_intent  \n",
       "195907  information_intent  \n",
       "195972  information_intent  \n",
       "\n",
       "[1072 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.loc[df['sequence'].apply(lambda text: len(text) > 64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e0d5f3-b927-477e-ac5b-5e05fce4835b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    sequence  token_length\n",
      "0                               Breaking Bad             4\n",
      "1                  what is japanese knotweed             7\n",
      "2               what framing square used for             7\n",
      "3       diseases that are caused by bacteria             8\n",
      "4             how to register on expedia.com            11\n",
      "...                                      ...           ...\n",
      "196247     chase custom finance phone number             7\n",
      "196248                     burrow definition             5\n",
      "196249         calories in riced cauliflower            10\n",
      "196250           Best bars in Cape Girardeau             9\n",
      "196251                   define incapacitate             6\n",
      "\n",
      "[196252 rows x 2 columns]\n",
      "Max token length: 55\n",
      "Average token length: 8.146332266677536\n",
      "90th percentile token length: 11.0\n",
      "95th percentile token length: 13.0\n",
      "98th percentile token length: 14.0\n",
      "99th percentile token length: 15.0\n",
      "99.5th percentile token length: 16.0\n",
      "99.9th percentile token length: 21.0\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "token_lengths = []\n",
    "for sequence in df['sequence'].values:\n",
    "    tokens = tokenizer(sequence, truncation=False)['input_ids']  # Get tokenized input IDs\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "temp_df = pd.DataFrame({'sequence': df['sequence'].values, 'token_length': token_lengths})\n",
    "\n",
    "# Display token lengths\n",
    "print(temp_df)\n",
    "\n",
    "# Optional: Analyze token lengths for deciding the best max_length\n",
    "print(f\"Max token length: {temp_df['token_length'].max()}\")\n",
    "print(f\"Average token length: {temp_df['token_length'].mean()}\")\n",
    "print(f\"90th percentile token length: {temp_df['token_length'].quantile(0.9)}\")\n",
    "print(f\"95th percentile token length: {temp_df['token_length'].quantile(0.95)}\")\n",
    "print(f\"98th percentile token length: {temp_df['token_length'].quantile(0.98)}\")\n",
    "print(f\"99th percentile token length: {temp_df['token_length'].quantile(0.99)}\")\n",
    "print(f\"99.5th percentile token length: {temp_df['token_length'].quantile(0.995)}\")\n",
    "print(f\"99.9th percentile token length: {temp_df['token_length'].quantile(0.999)}\")\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e6c934-fdc6-4d28-9c87-69eec74e5beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    120902\n",
      "1     28290\n",
      "2     14332\n",
      "5     13750\n",
      "4      7893\n",
      "3      6686\n",
      "6      2294\n",
      "7      2105\n",
      "Name: count, dtype: int64\n",
      "Size of sampled_df = 196252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_54611/1383443563.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('target', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what are the iliopsoas muscles</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is kratom herb</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>natural remedies for constipation due to medications</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why did queen elizabeth 1 have mary 1 executed</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is a hostel</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence              target  \\\n",
       "0                        what are the iliopsoas muscles  information_intent   \n",
       "1                                   what is kratom herb  information_intent   \n",
       "2  natural remedies for constipation due to medications  information_intent   \n",
       "3        why did queen elizabeth 1 have mary 1 executed  information_intent   \n",
       "4                                      what is a hostel  information_intent   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only a sample from the actual data\n",
    "\n",
    "sampling_percentages = {\n",
    "    'information_intent': 1.0,   # 100% sampling for information_intent\n",
    "    'yelp_intent': 1.0,          # 100% sampling for yelp_intent\n",
    "    'weather_intent': 1.0,       # 100% sampling for weather_intent\n",
    "    'navigation_intent': 1.0,    # 100% sampling for navigation_intent\n",
    "    'purchase_intent': 1.0,      # 100% sampling for purchase_intent\n",
    "    'translation_intent': 1.0,   # 100% sampling for translation_intent\n",
    "    'travel_intent': 1.0,        # 100% sampling for travel_intent\n",
    "    'unknown': 1.0               # 100% sampling for unknown\n",
    "}\n",
    "\n",
    "# Sample from each target group based on the defined percentages\n",
    "sampled_df = df.groupby('target', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=sampling_percentages.get(x.name, 1.0))\n",
    ").reset_index(drop=True)\n",
    "\n",
    "sampled_df['label'] = sampled_df['target'].map(label2id)\n",
    "# sampled_df = sampled_df.rename(columns={'target': 'label'})\n",
    "\n",
    "print(sampled_df['label'].value_counts())\n",
    "print(f\"Size of sampled_df = {len(sampled_df)}\")\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dadb9a2-6184-4e50-bf21-8063e31edbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sequence', 'target', 'label'],\n",
      "        num_rows: 186439\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sequence', 'target', 'label'],\n",
      "        num_rows: 9813\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the DataFrame into train and validation sets\n",
    "train_df, val_df = train_test_split(sampled_df, test_size=0.05, random_state=42, stratify=sampled_df['label'])\n",
    "\n",
    "# Step 2: Convert Pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "# Step 3: Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "# Step 4: Verify the structure of DatasetDict\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0613b1e7-e4ec-4b18-91a2-2648c42a91b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    114857\n",
       "1     26875\n",
       "2     13615\n",
       "5     13063\n",
       "4      7498\n",
       "3      6352\n",
       "6      2179\n",
       "7      2000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d476d6-4aa2-4180-889b-49f8e5251219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    6045\n",
       "1    1415\n",
       "2     717\n",
       "5     687\n",
       "4     395\n",
       "3     334\n",
       "6     115\n",
       "7     105\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae60f7-3f28-4d95-bee7-28d05b68566f",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17cb2d99-4f2f-4caa-8f1f-747fbc9c9082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"sequence\"]\n",
    "\n",
    "    # tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"right\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,  # Pad the sequences to the longest in the batch\n",
    "        max_length=64\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d52d8863-7705-40d2-a6b4-26b1865f5c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 186439/186439 [00:07<00:00, 23831.70 examples/s]\n",
      "Map: 100%|██████████| 9813/9813 [00:00<00:00, 25407.47 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'target', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 186439\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'target', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9813\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "# tokenized_dataset = tokenized_dataset.map(fix_labels)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56166c5b-958c-4921-961a-7e42c1ef37d7",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d481d862-dfe6-4635-9d0c-7c827a2155f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f08d8464-0714-4021-a266-9ceb64f36f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "floor repair cost -> yelp_intent\n",
      "denture fix -> yelp_intent\n",
      "who is the us president -> yelp_intent\n",
      "italian food -> yelp_intent\n",
      "sandwiches in seattle -> yelp_intent\n",
      "seattle weather -> yelp_intent\n",
      "weather seattle -> yelp_intent\n",
      "boston wether -> yelp_intent\n",
      "Boston wether -> yelp_intent\n",
      "weather boston -> yelp_intent\n",
      "weather Boston -> yelp_intent\n",
      "Weather Boston -> yelp_intent\n",
      "weathr boston -> yelp_intent\n",
      "seattle weathr -> yelp_intent\n",
      "apple macbook price -> yelp_intent\n",
      "sf sushi -> yelp_intent\n",
      "sf ramen -> yelp_intent\n",
      "seattle sushi -> yelp_intent\n",
      "seattle ramen -> yelp_intent\n",
      "sushi sf -> yelp_intent\n",
      "ramen sf -> yelp_intent\n",
      "chase bank login -> yelp_intent\n",
      "passport application -> yelp_intent\n",
      "walm -> yelp_intent\n",
      "footbal -> yelp_intent\n",
      "movers quote -> yelp_intent\n",
      "its sushi -> yelp_intent\n",
      "average house cleaning rates -> yelp_intent\n",
      "fun things to do -> yelp_intent\n",
      "foster gwin -> yelp_intent\n",
      "go karts -> yelp_intent\n",
      "facial -> yelp_intent\n",
      "donuts -> yelp_intent\n",
      "via carota -> yelp_intent\n",
      "tomi jazz -> yelp_intent\n",
      "mun korean steakhouse -> yelp_intent\n",
      "la pecora bianca -> yelp_intent\n",
      "birria tacos -> yelp_intent\n",
      "bank of america -> yelp_intent\n",
      "bungee fitness -> yelp_intent\n",
      "allien -> yelp_intent\n",
      "proposa -> yelp_intent\n",
      "wather -> yelp_intent\n",
      "big city -> yelp_intent\n",
      "orlando bloom -> yelp_intent\n",
      "banana -> yelp_intent\n",
      "lenght -> yelp_intent\n"
     ]
    }
   ],
   "source": [
    "### Evaluate untrained model\n",
    "\n",
    "text_list = [\n",
    "    'floor repair cost',\n",
    "    'denture fix',\n",
    "    'who is the us president',\n",
    "    'italian food',\n",
    "    'sandwiches in seattle',\n",
    "    'seattle weather',\n",
    "    'weather seattle',\n",
    "    'boston wether',\n",
    "    'Boston wether',\n",
    "    'weather boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weathr boston',\n",
    "    'seattle weathr',\n",
    "    'apple macbook price',\n",
    "    'sf sushi',\n",
    "    'sf ramen',\n",
    "    'seattle sushi',\n",
    "    'seattle ramen',\n",
    "    'sushi sf',\n",
    "    'ramen sf',\n",
    "    'chase bank login',\n",
    "    'passport application',\n",
    "    'walm',\n",
    "    'footbal',\n",
    "    'movers quote','its sushi', 'average house cleaning rates', 'fun things to do', 'foster gwin', 'go karts', 'facial', 'donuts',\n",
    "    'via carota', 'tomi jazz', 'mun korean steakhouse', 'la pecora bianca', 'birria tacos', 'bank of america', 'bungee fitness',\n",
    "    'allien', 'proposa', 'wather', 'big city', 'orlando bloom', 'banana', 'lenght',\n",
    "]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "predictions = []\n",
    "logits_list = []\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    logits = model(inputs).logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    predictions.append(prediction)\n",
    "    print(text + \" -> \" + id2label[prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfb5ad-391d-42da-bcf5-d62e39642225",
   "metadata": {},
   "source": [
    "#### Model finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cece8cd8-b017-4f80-9d9a-713041270fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 200,712 || all params: 24,786,704 || trainable%: 0.8098\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                         r=16, # intrinsic rank of trainable weight matrix\n",
    "                         lora_alpha=32, # similar to learning_rate\n",
    "                         lora_dropout=0.01, # probability of dropout nodes\n",
    "                         target_modules=['attention.self.query', 'attention.self.key']) # LoRA is applied to query layer\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c9ac1c-cd57-4173-b208-78831d9c4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccdcde-6b2c-499d-a185-d04dbd1b6ba7",
   "metadata": {},
   "source": [
    "#### Define hyper parameters and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84297802-ae8a-4fef-882a-7d7145de72bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "num_epochs = 12\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=model_checkpoint + \"-lora-intent-classification-v3\",\n",
    "    output_dir=\"mobilebert-uncased\" + \"-lora-intent-classification-v6\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # warmup_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "615cdb83-edc7-409a-a929-6c370c8aa9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, # Hyperparamaters\n",
    "    train_dataset=tokenized_dataset[\"train\"], # training data\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # validation data\n",
    "    tokenizer=tokenizer, # tokenizer\n",
    "    data_collator=data_collator, # dynamic sequence padding\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9fde457-1a0d-4552-b6fa-1c0642349972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46616' max='69924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46616/69924 1:29:58 < 44:59, 8.63 it/s, Epoch 8/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.634600</td>\n",
       "      <td>3.573355</td>\n",
       "      <td>0.885560</td>\n",
       "      <td>0.887858</td>\n",
       "      <td>0.885560</td>\n",
       "      <td>0.882967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>40.045600</td>\n",
       "      <td>2.440991</td>\n",
       "      <td>0.923061</td>\n",
       "      <td>0.930207</td>\n",
       "      <td>0.923061</td>\n",
       "      <td>0.925753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>30.031800</td>\n",
       "      <td>0.907024</td>\n",
       "      <td>0.942933</td>\n",
       "      <td>0.941680</td>\n",
       "      <td>0.942933</td>\n",
       "      <td>0.941776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.083200</td>\n",
       "      <td>0.602224</td>\n",
       "      <td>0.948436</td>\n",
       "      <td>0.947712</td>\n",
       "      <td>0.948436</td>\n",
       "      <td>0.947804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.150300</td>\n",
       "      <td>0.179747</td>\n",
       "      <td>0.952716</td>\n",
       "      <td>0.952055</td>\n",
       "      <td>0.952716</td>\n",
       "      <td>0.952296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>16.257400</td>\n",
       "      <td>0.194166</td>\n",
       "      <td>0.954041</td>\n",
       "      <td>0.953524</td>\n",
       "      <td>0.954041</td>\n",
       "      <td>0.953687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>9.919100</td>\n",
       "      <td>0.507980</td>\n",
       "      <td>0.955671</td>\n",
       "      <td>0.955129</td>\n",
       "      <td>0.955671</td>\n",
       "      <td>0.955230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>11.940600</td>\n",
       "      <td>2.557006</td>\n",
       "      <td>0.957302</td>\n",
       "      <td>0.957090</td>\n",
       "      <td>0.957302</td>\n",
       "      <td>0.957142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=46616, training_loss=5480.535536407463, metrics={'train_runtime': 5398.9449, 'train_samples_per_second': 414.39, 'train_steps_per_second': 12.951, 'total_flos': 8005097728964160.0, 'train_loss': 5480.535536407463, 'epoch': 8.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1da3c5f1-ef7f-4ca1-9792-4879b621f267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15d874c9-e652-4c17-8a54-11d94710debb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floor repair cost -> yelp_intent\n",
      "denture fix -> yelp_intent\n",
      "who is the us president -> information_intent\n",
      "italian food -> yelp_intent\n",
      "sandwiches in seattle -> yelp_intent\n",
      "seattle weather -> weather_intent\n",
      "weather seattle -> weather_intent\n",
      "boston wether -> weather_intent\n",
      "Boston wether -> weather_intent\n",
      "weather boston -> weather_intent\n",
      "weather Boston -> weather_intent\n",
      "Weather Boston -> weather_intent\n",
      "weathr boston -> weather_intent\n",
      "seattle weathr -> weather_intent\n",
      "apple macbook price -> purchase_intent\n",
      "sf sushi -> yelp_intent\n",
      "sf ramen -> yelp_intent\n",
      "seattle sushi -> yelp_intent\n",
      "seattle ramen -> yelp_intent\n",
      "sushi sf -> yelp_intent\n",
      "ramen sf -> yelp_intent\n",
      "chase bank login -> navigation_intent\n",
      "passport application -> navigation_intent\n",
      "walm -> unknown\n",
      "footbal -> yelp_intent\n",
      "movers quote -> yelp_intent\n",
      "its sushi -> yelp_intent\n",
      "average house cleaning rates -> yelp_intent\n",
      "fun things to do -> yelp_intent\n",
      "foster gwin -> yelp_intent\n",
      "go karts -> yelp_intent\n",
      "facial -> information_intent\n",
      "donuts -> yelp_intent\n",
      "via carota -> travel_intent\n",
      "tomi jazz -> purchase_intent\n",
      "mun korean steakhouse -> yelp_intent\n",
      "la pecora bianca -> yelp_intent\n",
      "birria tacos -> travel_intent\n",
      "bank of america -> yelp_intent\n",
      "bungee fitness -> yelp_intent\n",
      "allien -> yelp_intent\n",
      "proposa -> yelp_intent\n",
      "wather -> weather_intent\n",
      "big city -> yelp_intent\n",
      "orlando bloom -> travel_intent\n",
      "banana -> yelp_intent\n",
      "lenght -> unknown\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in text_list:\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "        logits = trainer.model(inputs).logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        print(text + \" -> \" + id2label[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66aae059-b207-43d5-8a78-a76901550c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.4M\n",
      "-rw-r--r-- 1 jupyter jupyter 5.0K Nov 20 05:24 README.md\n",
      "-rw-r--r-- 1 jupyter jupyter  706 Nov 20 05:24 adapter_config.json\n",
      "-rw-r--r-- 1 jupyter jupyter 798K Nov 20 05:24 adapter_model.safetensors\n",
      "-rw-r--r-- 1 jupyter jupyter 1.7M Nov 20 05:24 optimizer.pt\n",
      "-rw-r--r-- 1 jupyter jupyter  14K Nov 20 05:24 rng_state.pth\n",
      "-rw-r--r-- 1 jupyter jupyter 1.1K Nov 20 05:24 scheduler.pt\n",
      "-rw-r--r-- 1 jupyter jupyter  125 Nov 20 05:24 special_tokens_map.json\n",
      "-rw-r--r-- 1 jupyter jupyter 695K Nov 20 05:24 tokenizer.json\n",
      "-rw-r--r-- 1 jupyter jupyter 1.3K Nov 20 05:24 tokenizer_config.json\n",
      "-rw-r--r-- 1 jupyter jupyter  20K Nov 20 05:24 trainer_state.json\n",
      "-rw-r--r-- 1 jupyter jupyter 5.2K Nov 20 05:24 training_args.bin\n",
      "-rw-r--r-- 1 jupyter jupyter 227K Nov 20 05:24 vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -lh mobilebert-uncased-lora-intent-classification-v6/checkpoint-46616"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08157f-ff0c-46a3-aa0d-f04493a71d36",
   "metadata": {},
   "source": [
    "#### Load the LoRA model from checkpoint after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "041cee58-787b-430c-a9c4-c4aadcf87a58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -3.2607,   7.3945,  -4.3247,  -8.0733,   1.8455,  -4.7625, -10.6814,\n",
      "          -5.6815]], grad_fn=<AddmmBackward0>)\n",
      "1 yelp_intent\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'information_intent',\n",
    "            1: 'yelp_intent',\n",
    "            2: 'navigation_intent',\n",
    "            3: 'travel_intent',\n",
    "            4: 'purchase_intent',\n",
    "            5: 'weather_intent',\n",
    "            6: 'translation_intent',\n",
    "            7: 'unknown'}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "\n",
    "output_dir = \"mobilebert-uncased-lora-intent-classification-v6/checkpoint-46616\"\n",
    "\n",
    "# Load the tokenizer (from the output directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Load the base model from the original checkpoint (base pre-trained model)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Load the LoRA configuration and model\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Step 3: Save the combined model to a directory\n",
    "save_directory = \"tmp/mobilebert_lora_combined_model/\"\n",
    "lora_model.save_pretrained(save_directory)  # Save base model + LoRA weights\n",
    "\n",
    "# Now the `lora_model` contains both the base model and the LoRA weights.\n",
    "lora_model.eval()\n",
    "\n",
    "# Example inference\n",
    "inputs = tokenizer([\"looking for home cleaning \"], return_tensors=\"pt\")\n",
    "outputs = lora_model(**inputs)\n",
    "logits = outputs.logits\n",
    "print(logits)\n",
    "\n",
    "\n",
    "prediction = torch.argmax(logits, dim=1).item()\n",
    "print(prediction, id2label[prediction])\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "rounded_probabilities = torch.round(probabilities)\n",
    "print(rounded_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5f259a5-9032-4614-b0fc-b6c685ea250e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tmp/mobilebert_lora_combined_model/tokenizer_config.json',\n",
       " 'tmp/mobilebert_lora_combined_model/special_tokens_map.json',\n",
       " 'tmp/mobilebert_lora_combined_model/vocab.txt',\n",
       " 'tmp/mobilebert_lora_combined_model/added_tokens.json',\n",
       " 'tmp/mobilebert_lora_combined_model/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Step 1: Load the base model (DistilBERT)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Step 2: Load the LoRA adapter weights\n",
    "output_dir = \"mobilebert-uncased-lora-intent-classification-v6/checkpoint-46616\"\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Step 3: Merge LoRA weights into the base model\n",
    "# After this, the model will have both base and LoRA weights applied\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Step 4: Save the full model (base model + LoRA weights)\n",
    "save_directory = \"tmp/mobilebert_lora_combined_model/\"\n",
    "merged_model.save_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)  # Load the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)  # Save the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eb33ea5-05c4-4417-8c86-05782471d680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75982821-faaf-45ba-9936-26f1c275f78f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 98.5M/98.5M [00:03<00:00, 26.2MB/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier/commit/268e37cb720741e2eee30bbc01005798432cf545', commit_message='Upload tokenizer', commit_description='', oid='268e37cb720741e2eee30bbc01005798432cf545', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_dir = \"tmp/mobilebert_lora_combined_model\"\n",
    "merged_repo_id = \"Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier\"  \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1bcba70-77f1-493e-8c35-e135ec2f9893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ce05e-44be-4fc1-b8bb-ada9f1415386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
