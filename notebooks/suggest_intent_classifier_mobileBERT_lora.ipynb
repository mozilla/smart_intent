{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83df52ac-a343-49ec-83bc-16b8c91b9dd0",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to use LORA (aka Low Rank Adaptation method) and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86420020-5a69-4a3b-a6be-6ce274be8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -q datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54679b61-b9f8-4a5c-a846-0f11a1946d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip uninstall -y pyarrow datasets ibis-framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bd4be8-9789-447d-880a-264763f117fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyarrow>=15.0.0 datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05e4a8e-313a-48e5-9cb6-16ec108133bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip show pyarrow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a5ba79-b2c6-4ab7-b0dd-940eeab27cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyarrow>=15.0.0 datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c683dd-30bd-448a-ada9-bc4562b61663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                         AutoConfig,\n",
    "                         AutoModelForSequenceClassification,\n",
    "                         DataCollatorWithPadding,\n",
    "                         TrainingArguments,\n",
    "                         Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32ae2ac-7f08-4fc4-baa7-10a7ccbcf800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e512cf-4776-4278-9612-ec5c8be44f80",
   "metadata": {},
   "source": [
    "#### Base Model (mobileBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1dc1475-55e2-42ca-a881-8f33e475f41c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'google/mobilebert-uncased'\n",
    "id2label = {0: 'information_intent',\n",
    "            1: 'yelp_intent',\n",
    "            2: 'navigation_intent',\n",
    "            3: 'travel_intent',\n",
    "            4: 'purchase_intent',\n",
    "            5: 'weather_intent',\n",
    "            6: 'translation_intent',\n",
    "            7: 'unknown'}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "\n",
    "# generate classification model from model chckpoints\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=8,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeaa99b-10c2-44ed-8d21-08ed2b3990bd",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81eb7dac-565f-4247-b42d-fe2d7675ec60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176568\n",
      "target\n",
      "information_intent    117561\n",
      "yelp_intent            28489\n",
      "weather_intent         13271\n",
      "navigation_intent      11532\n",
      "purchase_intent         2220\n",
      "translation_intent      2159\n",
      "travel_intent           1320\n",
      "unknown                   16\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cost of hardwood flooring installed</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cost of full mouth dental implants</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cost of bmw suv</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how much money do writers make</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>average temperature shanghai</td>\n",
       "      <td>weather_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>walt disney fast pass cost</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>villanova tuition cost</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cost of floor to ceiling windows</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cost of deka batteries</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>current stock price of micron</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              sequence              target\n",
       "0  cost of hardwood flooring installed         yelp_intent\n",
       "1   cost of full mouth dental implants         yelp_intent\n",
       "2                      cost of bmw suv  information_intent\n",
       "3       how much money do writers make  information_intent\n",
       "4         average temperature shanghai      weather_intent\n",
       "5           walt disney fast pass cost  information_intent\n",
       "6               villanova tuition cost  information_intent\n",
       "7     cost of floor to ceiling windows         yelp_intent\n",
       "8               cost of deka batteries  information_intent\n",
       "9        current stock price of micron  information_intent"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/marco_train_v4.csv\")\n",
    "print(len(df))\n",
    "print(df['target'].value_counts())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb24484d-7380-49b6-b67f-5b645e46cf49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    176568.000000\n",
      "mean         29.835027\n",
      "std          10.651826\n",
      "min           6.000000\n",
      "10%          18.000000\n",
      "20%          21.000000\n",
      "25%          22.000000\n",
      "30%          24.000000\n",
      "40%          26.000000\n",
      "50%          29.000000\n",
      "60%          31.000000\n",
      "70%          34.000000\n",
      "75%          36.000000\n",
      "80%          37.000000\n",
      "90%          43.000000\n",
      "95%          48.000000\n",
      "98%          55.000000\n",
      "99%          61.000000\n",
      "99.5%        69.000000\n",
      "99.8%        83.000000\n",
      "99.9%        95.000000\n",
      "max         193.000000\n",
      "Name: sequence, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df['sequence'].apply(len).describe(percentiles=[.1, .2, .25, .3, .4, .5, .6, .7, .75, .8, .9, .95, .98, .99, .995, .998, .999]))\n",
    "df['sequence'].apply(len).hist(bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "082fb996-037b-4353-8739-7047d6142f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>what is the difference between marginal cost and marginal revenue</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>price of four day ticket for florida resident to disney world in florida</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21766</th>\n",
       "      <td>Top stores offering deals on Asics running shoesWhere to buy {gifts} online?</td>\n",
       "      <td>purchase_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21776</th>\n",
       "      <td>Top stores offering deals on Bauer hockey skatesWhere to buy {gifts} online?</td>\n",
       "      <td>purchase_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21796</th>\n",
       "      <td>Compare Hoover carpet cleaner with Hoover carpet cleaner for best value</td>\n",
       "      <td>purchase_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176151</th>\n",
       "      <td>__________ is the process of hiring another organization to perform a service.</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176155</th>\n",
       "      <td>requirements to be awarded global war on terrorism service medal army</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176170</th>\n",
       "      <td>price is the _____________ a consumer is willing to make to acquire a specific product or servic...</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176190</th>\n",
       "      <td>lean six sigma for service is a business improvement methodology that maximizes</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176219</th>\n",
       "      <td>are the federal public service loan forgiveness and teacher loan forgiveness different</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1227 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   sequence  \\\n",
       "756                                       what is the difference between marginal cost and marginal revenue   \n",
       "1165                               price of four day ticket for florida resident to disney world in florida   \n",
       "21766                          Top stores offering deals on Asics running shoesWhere to buy {gifts} online?   \n",
       "21776                          Top stores offering deals on Bauer hockey skatesWhere to buy {gifts} online?   \n",
       "21796                               Compare Hoover carpet cleaner with Hoover carpet cleaner for best value   \n",
       "...                                                                                                     ...   \n",
       "176151                       __________ is the process of hiring another organization to perform a service.   \n",
       "176155                                requirements to be awarded global war on terrorism service medal army   \n",
       "176170  price is the _____________ a consumer is willing to make to acquire a specific product or servic...   \n",
       "176190                      lean six sigma for service is a business improvement methodology that maximizes   \n",
       "176219               are the federal public service loan forgiveness and teacher loan forgiveness different   \n",
       "\n",
       "                    target  \n",
       "756     information_intent  \n",
       "1165    information_intent  \n",
       "21766      purchase_intent  \n",
       "21776      purchase_intent  \n",
       "21796      purchase_intent  \n",
       "...                    ...  \n",
       "176151         yelp_intent  \n",
       "176155         yelp_intent  \n",
       "176170         yelp_intent  \n",
       "176190         yelp_intent  \n",
       "176219         yelp_intent  \n",
       "\n",
       "[1227 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.loc[df['sequence'].apply(lambda text: len(text) > 64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e0d5f3-b927-477e-ac5b-5e05fce4835b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                sequence  token_length\n",
      "0                    cost of hardwood flooring installed             8\n",
      "1                     cost of full mouth dental implants             9\n",
      "2                                        cost of bmw suv             6\n",
      "3                         how much money do writers make             8\n",
      "4                           average temperature shanghai             5\n",
      "...                                                  ...           ...\n",
      "176563  does a fire stick work without an amazon account            11\n",
      "176564               what apps can be put on amazon fire            10\n",
      "176565                 amazon customer care number india             7\n",
      "176566                            price per share amazon             6\n",
      "176567                         how much does amazon pay?             8\n",
      "\n",
      "[176568 rows x 2 columns]\n",
      "Max token length: 55\n",
      "Average token length: 8.333531557247067\n",
      "90th percentile token length: 11.0\n",
      "95th percentile token length: 13.0\n",
      "98th percentile token length: 14.0\n",
      "99th percentile token length: 15.0\n",
      "99.5th percentile token length: 17.0\n",
      "99.9th percentile token length: 22.0\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "token_lengths = []\n",
    "for sequence in df['sequence'].values:\n",
    "    tokens = tokenizer(sequence, truncation=False)['input_ids']  # Get tokenized input IDs\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "temp_df = pd.DataFrame({'sequence': df['sequence'].values, 'token_length': token_lengths})\n",
    "\n",
    "# Display token lengths\n",
    "print(temp_df)\n",
    "\n",
    "# Optional: Analyze token lengths for deciding the best max_length\n",
    "print(f\"Max token length: {temp_df['token_length'].max()}\")\n",
    "print(f\"Average token length: {temp_df['token_length'].mean()}\")\n",
    "print(f\"90th percentile token length: {temp_df['token_length'].quantile(0.9)}\")\n",
    "print(f\"95th percentile token length: {temp_df['token_length'].quantile(0.95)}\")\n",
    "print(f\"98th percentile token length: {temp_df['token_length'].quantile(0.98)}\")\n",
    "print(f\"99th percentile token length: {temp_df['token_length'].quantile(0.99)}\")\n",
    "print(f\"99.5th percentile token length: {temp_df['token_length'].quantile(0.995)}\")\n",
    "print(f\"99.9th percentile token length: {temp_df['token_length'].quantile(0.999)}\")\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e6c934-fdc6-4d28-9c87-69eec74e5beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    47024\n",
      "1    28489\n",
      "5    13271\n",
      "2    11532\n",
      "4     2220\n",
      "6     2159\n",
      "3     1320\n",
      "7       16\n",
      "Name: count, dtype: int64\n",
      "Size of sampled_df = 106031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_513262/2457747801.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('target', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is ecological approach</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what blood types are there</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what are crabs</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aquarius meanings</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>definition of appurtenances</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sequence              target  label\n",
       "0  what is ecological approach  information_intent      0\n",
       "1   what blood types are there  information_intent      0\n",
       "2               what are crabs  information_intent      0\n",
       "3            aquarius meanings  information_intent      0\n",
       "4  definition of appurtenances  information_intent      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only a sample from the actual data\n",
    "\n",
    "sampling_percentages = {\n",
    "    'information_intent': 0.4,   # 40% sampling for information_intent\n",
    "    'yelp_intent': 1.0,          # 100% sampling for yelp_intent\n",
    "    'weather_intent': 1.0,       # 100% sampling for weather_intent\n",
    "    'navigation_intent': 1.0,    # 100% sampling for navigation_intent\n",
    "    'purchase_intent': 1.0,      # 100% sampling for purchase_intent\n",
    "    'translation_intent': 1.0,   # 100% sampling for translation_intent\n",
    "    'travel_intent': 1.0,        # 100% sampling for travel_intent\n",
    "    'unknown': 1.0               # 100% sampling for unknown\n",
    "}\n",
    "\n",
    "# Sample from each target group based on the defined percentages\n",
    "sampled_df = df.groupby('target', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=sampling_percentages.get(x.name, 1.0))\n",
    ").reset_index(drop=True)\n",
    "\n",
    "sampled_df['label'] = sampled_df['target'].map(label2id)\n",
    "# sampled_df = sampled_df.rename(columns={'target': 'label'})\n",
    "\n",
    "print(sampled_df['label'].value_counts())\n",
    "print(f\"Size of sampled_df = {len(sampled_df)}\")\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dadb9a2-6184-4e50-bf21-8063e31edbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sequence', 'target', 'label'],\n",
      "        num_rows: 100729\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sequence', 'target', 'label'],\n",
      "        num_rows: 5302\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the DataFrame into train and validation sets\n",
    "train_df, val_df = train_test_split(sampled_df, test_size=0.05, random_state=42, stratify=sampled_df['label'])\n",
    "\n",
    "# Step 2: Convert Pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "# Step 3: Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "# Step 4: Verify the structure of DatasetDict\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0613b1e7-e4ec-4b18-91a2-2648c42a91b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    44673\n",
       "1    27065\n",
       "5    12607\n",
       "2    10955\n",
       "4     2109\n",
       "6     2051\n",
       "3     1254\n",
       "7       15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d476d6-4aa2-4180-889b-49f8e5251219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    2351\n",
       "1    1424\n",
       "5     664\n",
       "2     577\n",
       "4     111\n",
       "6     108\n",
       "3      66\n",
       "7       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae60f7-3f28-4d95-bee7-28d05b68566f",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17cb2d99-4f2f-4caa-8f1f-747fbc9c9082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"sequence\"]\n",
    "\n",
    "    # tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"right\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,  # Pad the sequences to the longest in the batch\n",
    "        max_length=64\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d52d8863-7705-40d2-a6b4-26b1865f5c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100729/100729 [00:04<00:00, 23286.10 examples/s]\n",
      "Map: 100%|██████████| 5302/5302 [00:00<00:00, 27768.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'target', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 100729\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'target', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5302\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "# tokenized_dataset = tokenized_dataset.map(fix_labels)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56166c5b-958c-4921-961a-7e42c1ef37d7",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d481d862-dfe6-4635-9d0c-7c827a2155f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f08d8464-0714-4021-a266-9ceb64f36f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "floor repair cost -> yelp_intent\n",
      "denture fix -> yelp_intent\n",
      "who is the us president -> yelp_intent\n",
      "italian food -> yelp_intent\n",
      "sandwiches in seattle -> yelp_intent\n",
      "seattle weather -> yelp_intent\n",
      "weather seattle -> yelp_intent\n",
      "boston wether -> yelp_intent\n",
      "Boston wether -> yelp_intent\n",
      "weather boston -> yelp_intent\n",
      "weather Boston -> yelp_intent\n",
      "Weather Boston -> yelp_intent\n",
      "weathr boston -> yelp_intent\n",
      "seattle weathr -> yelp_intent\n",
      "apple macbook price -> yelp_intent\n",
      "sf sushi -> yelp_intent\n",
      "sf ramen -> yelp_intent\n",
      "seattle sushi -> yelp_intent\n",
      "seattle ramen -> yelp_intent\n",
      "sushi sf -> yelp_intent\n",
      "ramen sf -> yelp_intent\n",
      "chase bank login -> yelp_intent\n",
      "passport application -> yelp_intent\n"
     ]
    }
   ],
   "source": [
    "### Evaluate untrained model\n",
    "\n",
    "text_list = [\n",
    "    'floor repair cost',\n",
    "    'denture fix',\n",
    "    'who is the us president',\n",
    "    'italian food',\n",
    "    'sandwiches in seattle',\n",
    "    'seattle weather',\n",
    "    'weather seattle',\n",
    "    'boston wether',\n",
    "    'Boston wether',\n",
    "    'weather boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weathr boston',\n",
    "    'seattle weathr',\n",
    "    'apple macbook price',\n",
    "    'sf sushi',\n",
    "    'sf ramen',\n",
    "    'seattle sushi',\n",
    "    'seattle ramen',\n",
    "    'sushi sf',\n",
    "    'ramen sf',\n",
    "    'chase bank login',\n",
    "    'passport application'\n",
    "]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "predictions = []\n",
    "logits_list = []\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    logits = model(inputs).logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    predictions.append(prediction)\n",
    "    print(text + \" -> \" + id2label[prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfb5ad-391d-42da-bcf5-d62e39642225",
   "metadata": {},
   "source": [
    "#### Model finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cece8cd8-b017-4f80-9d9a-713041270fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 53,256 || all params: 24,639,248 || trainable%: 0.2161\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                         r=8, # intrinsic rank of trainable weight matrix\n",
    "                         lora_alpha=32, # similar to learning_rate\n",
    "                         lora_dropout=0.01, # probability of dropout nodes\n",
    "                         target_modules=['attention.self.query']) # LoRA is applied to query layer\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93c9ac1c-cd57-4173-b208-78831d9c4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccdcde-6b2c-499d-a185-d04dbd1b6ba7",
   "metadata": {},
   "source": [
    "#### Define hyper parameters and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84297802-ae8a-4fef-882a-7d7145de72bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "num_epochs = 12\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=model_checkpoint + \"-lora-intent-classification-v3\",\n",
    "    output_dir=\"mobilebert-uncased\" + \"-lora-intent-classification-v3\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # warmup_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615cdb83-edc7-409a-a929-6c370c8aa9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, # Hyperparamaters\n",
    "    train_dataset=tokenized_dataset[\"train\"], # training data\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # validation data\n",
    "    tokenizer=tokenizer, # tokenizer\n",
    "    data_collator=data_collator, # dynamic sequence padding\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9fde457-1a0d-4552-b6fa-1c0642349972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18888' max='37776' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18888/37776 33:37 < 33:38, 9.36 it/s, Epoch 6/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>29.346100</td>\n",
       "      <td>0.770477</td>\n",
       "      <td>0.888344</td>\n",
       "      <td>0.889029</td>\n",
       "      <td>0.888344</td>\n",
       "      <td>0.883857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>23.939200</td>\n",
       "      <td>0.591340</td>\n",
       "      <td>0.928895</td>\n",
       "      <td>0.929437</td>\n",
       "      <td>0.928895</td>\n",
       "      <td>0.927521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>21.278200</td>\n",
       "      <td>0.184457</td>\n",
       "      <td>0.944926</td>\n",
       "      <td>0.944547</td>\n",
       "      <td>0.944926</td>\n",
       "      <td>0.944497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15.879400</td>\n",
       "      <td>0.854668</td>\n",
       "      <td>0.948699</td>\n",
       "      <td>0.948812</td>\n",
       "      <td>0.948699</td>\n",
       "      <td>0.948557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>18.966500</td>\n",
       "      <td>0.402691</td>\n",
       "      <td>0.952282</td>\n",
       "      <td>0.952137</td>\n",
       "      <td>0.952282</td>\n",
       "      <td>0.952116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>19.564900</td>\n",
       "      <td>7.515117</td>\n",
       "      <td>0.956432</td>\n",
       "      <td>0.956289</td>\n",
       "      <td>0.956432</td>\n",
       "      <td>0.956220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18888, training_loss=9104.156293430353, metrics={'train_runtime': 2018.2685, 'train_samples_per_second': 598.903, 'train_steps_per_second': 18.717, 'total_flos': 3373306869000960.0, 'train_loss': 9104.156293430353, 'epoch': 6.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1da3c5f1-ef7f-4ca1-9792-4879b621f267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d874c9-e652-4c17-8a54-11d94710debb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floor repair cost -> yelp_intent\n",
      "denture fix -> yelp_intent\n",
      "who is the us president -> information_intent\n",
      "italian food -> yelp_intent\n",
      "sandwiches in seattle -> yelp_intent\n",
      "seattle weather -> weather_intent\n",
      "weather seattle -> weather_intent\n",
      "boston wether -> weather_intent\n",
      "Boston wether -> weather_intent\n",
      "weather boston -> weather_intent\n",
      "weather Boston -> weather_intent\n",
      "Weather Boston -> weather_intent\n",
      "weathr boston -> yelp_intent\n",
      "seattle weathr -> yelp_intent\n",
      "apple macbook price -> yelp_intent\n",
      "sf sushi -> yelp_intent\n",
      "sf ramen -> yelp_intent\n",
      "seattle sushi -> yelp_intent\n",
      "seattle ramen -> yelp_intent\n",
      "sushi sf -> yelp_intent\n",
      "ramen sf -> yelp_intent\n",
      "chase bank login -> navigation_intent\n",
      "passport application -> navigation_intent\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in text_list:\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "        logits = trainer.model(inputs).logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        print(text + \" -> \" + id2label[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66aae059-b207-43d5-8a78-a76901550c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.7M\n",
      "-rw-r--r-- 1 jupyter jupyter 5.0K Oct 27 17:44 README.md\n",
      "-rw-r--r-- 1 jupyter jupyter  679 Oct 27 17:44 adapter_config.json\n",
      "-rw-r--r-- 1 jupyter jupyter 215K Oct 27 17:44 adapter_model.safetensors\n",
      "-rw-r--r-- 1 jupyter jupyter 458K Oct 27 17:44 optimizer.pt\n",
      "-rw-r--r-- 1 jupyter jupyter  14K Oct 27 17:44 rng_state.pth\n",
      "-rw-r--r-- 1 jupyter jupyter 1.1K Oct 27 17:44 scheduler.pt\n",
      "-rw-r--r-- 1 jupyter jupyter  125 Oct 27 17:44 special_tokens_map.json\n",
      "-rw-r--r-- 1 jupyter jupyter 695K Oct 27 17:44 tokenizer.json\n",
      "-rw-r--r-- 1 jupyter jupyter 1.3K Oct 27 17:44 tokenizer_config.json\n",
      "-rw-r--r-- 1 jupyter jupyter 9.5K Oct 27 17:44 trainer_state.json\n",
      "-rw-r--r-- 1 jupyter jupyter 5.2K Oct 27 17:44 training_args.bin\n",
      "-rw-r--r-- 1 jupyter jupyter 227K Oct 27 17:44 vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -lh mobilebert-uncased-lora-intent-classification-v3/checkpoint-18888"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08157f-ff0c-46a3-aa0d-f04493a71d36",
   "metadata": {},
   "source": [
    "#### Load the LoRA model from checkpoint after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "041cee58-787b-430c-a9c4-c4aadcf87a58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3575,  5.0813,  0.1581, -2.5119,  0.0109, -4.0418, -6.2496, -6.2266]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "1 yelp_intent\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'information_intent',\n",
    "            1: 'yelp_intent',\n",
    "            2: 'navigation_intent',\n",
    "            3: 'travel_intent',\n",
    "            4: 'purchase_intent',\n",
    "            5: 'weather_intent',\n",
    "            6: 'translation_intent',\n",
    "            7: 'unknown'}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "\n",
    "output_dir = \"mobilebert-uncased-lora-intent-classification-v3/checkpoint-18888\"\n",
    "\n",
    "# Load the tokenizer (from the output directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Load the base model from the original checkpoint (base pre-trained model)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Load the LoRA configuration and model\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Step 3: Save the combined model to a directory\n",
    "save_directory = \"tmp/mobilebert_lora_combined_model/\"\n",
    "lora_model.save_pretrained(save_directory)  # Save base model + LoRA weights\n",
    "\n",
    "# Now the `lora_model` contains both the base model and the LoRA weights.\n",
    "lora_model.eval()\n",
    "\n",
    "# Example inference\n",
    "inputs = tokenizer([\"looking for home cleaning \"], return_tensors=\"pt\")\n",
    "outputs = lora_model(**inputs)\n",
    "logits = outputs.logits\n",
    "print(logits)\n",
    "\n",
    "\n",
    "prediction = torch.argmax(logits, dim=1).item()\n",
    "print(prediction, id2label[prediction])\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "rounded_probabilities = torch.round(probabilities)\n",
    "print(rounded_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5f259a5-9032-4614-b0fc-b6c685ea250e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tmp/mobilebert_lora_combined_model/tokenizer_config.json',\n",
       " 'tmp/mobilebert_lora_combined_model/special_tokens_map.json',\n",
       " 'tmp/mobilebert_lora_combined_model/vocab.txt',\n",
       " 'tmp/mobilebert_lora_combined_model/added_tokens.json',\n",
       " 'tmp/mobilebert_lora_combined_model/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Step 1: Load the base model (DistilBERT)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Step 2: Load the LoRA adapter weights\n",
    "output_dir = \"mobilebert-uncased-lora-intent-classification-v3/checkpoint-18888\"\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Step 3: Merge LoRA weights into the base model\n",
    "# After this, the model will have both base and LoRA weights applied\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Step 4: Save the full model (base model + LoRA weights)\n",
    "save_directory = \"tmp/mobilebert_lora_combined_model/\"\n",
    "merged_model.save_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)  # Load the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)  # Save the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eb33ea5-05c4-4417-8c86-05782471d680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75982821-faaf-45ba-9936-26f1c275f78f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 98.5M/98.5M [00:02<00:00, 42.9MB/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier/commit/337b7017dba1f1344847076d026e8e97d102d0f7', commit_message='Upload tokenizer', commit_description='', oid='337b7017dba1f1344847076d026e8e97d102d0f7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_dir = \"tmp/mobilebert_lora_combined_model\"\n",
    "merged_repo_id = \"Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier\"  \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1bcba70-77f1-493e-8c35-e135ec2f9893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ce05e-44be-4fc1-b8bb-ada9f1415386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
