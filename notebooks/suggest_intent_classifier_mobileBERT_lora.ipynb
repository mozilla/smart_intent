{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83df52ac-a343-49ec-83bc-16b8c91b9dd0",
   "metadata": {},
   "source": [
    "Purpose of this notebook is to use LORA (aka Low Rank Adaptation method) and finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86420020-5a69-4a3b-a6be-6ce274be8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -q datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54679b61-b9f8-4a5c-a846-0f11a1946d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip uninstall -y pyarrow datasets ibis-framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bd4be8-9789-447d-880a-264763f117fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyarrow>=15.0.0 datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05e4a8e-313a-48e5-9cb6-16ec108133bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip show pyarrow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a5ba79-b2c6-4ab7-b0dd-940eeab27cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyarrow>=15.0.0 datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c683dd-30bd-448a-ada9-bc4562b61663",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                         AutoConfig,\n",
    "                         AutoModelForSequenceClassification,\n",
    "                         DataCollatorWithPadding,\n",
    "                         TrainingArguments,\n",
    "                         Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32ae2ac-7f08-4fc4-baa7-10a7ccbcf800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e512cf-4776-4278-9612-ec5c8be44f80",
   "metadata": {},
   "source": [
    "#### Base Model (mobileBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1dc1475-55e2-42ca-a881-8f33e475f41c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'google/mobilebert-uncased'\n",
    "id2label = {0: 'information_intent',\n",
    "            1: 'yelp_intent',\n",
    "            2: 'navigation_intent',\n",
    "            3: 'travel_intent',\n",
    "            4: 'purchase_intent',\n",
    "            5: 'weather_intent',\n",
    "            6: 'translation_intent',\n",
    "            7: 'unknown'}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "\n",
    "# generate classification model from model chckpoints\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=8,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a893d3-2bf9-45cb-b678-436748405b94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileBertForSequenceClassification(\n",
       "  (mobilebert): MobileBertModel(\n",
       "    (embeddings): MobileBertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n",
       "      (LayerNorm): NoNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): MobileBertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x MobileBertLayer(\n",
       "          (attention): MobileBertAttention(\n",
       "            (self): MobileBertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): MobileBertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): NoNorm()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): MobileBertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): ReLU()\n",
       "          )\n",
       "          (output): MobileBertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): NoNorm()\n",
       "            (bottleneck): OutputBottleneck(\n",
       "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (LayerNorm): NoNorm()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (bottleneck): Bottleneck(\n",
       "            (input): BottleneckLayer(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): NoNorm()\n",
       "            )\n",
       "            (attention): BottleneckLayer(\n",
       "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (LayerNorm): NoNorm()\n",
       "            )\n",
       "          )\n",
       "          (ffn): ModuleList(\n",
       "            (0-2): 3 x FFNLayer(\n",
       "              (intermediate): MobileBertIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): ReLU()\n",
       "              )\n",
       "              (output): FFNOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (LayerNorm): NoNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): MobileBertPooler()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeaa99b-10c2-44ed-8d21-08ed2b3990bd",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81eb7dac-565f-4247-b42d-fe2d7675ec60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174823\n",
      "target\n",
      "information_intent    114685\n",
      "yelp_intent            38241\n",
      "weather_intent         13195\n",
      "navigation_intent       3498\n",
      "translation_intent      2159\n",
      "purchase_intent         1717\n",
      "travel_intent           1320\n",
      "unknown                    8\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what's the ingredients for simple syrup</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is the basic unit of mass in si</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fresno, CA packing supplies</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>does medicare pay for annual wellness visits</td>\n",
       "      <td>travel_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which is better: Samsung Galaxy or Samsung Gal...</td>\n",
       "      <td>purchase_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>where is clemson university graduation held</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what is the forcaste for lincoln &amp; hastings</td>\n",
       "      <td>weather_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>current weather in dallas</td>\n",
       "      <td>weather_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what is the time system</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>what is the strongest material</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence              target\n",
       "0            what's the ingredients for simple syrup  information_intent\n",
       "1               what is the basic unit of mass in si  information_intent\n",
       "2                        Fresno, CA packing supplies         yelp_intent\n",
       "3       does medicare pay for annual wellness visits       travel_intent\n",
       "4  Which is better: Samsung Galaxy or Samsung Gal...     purchase_intent\n",
       "5        where is clemson university graduation held  information_intent\n",
       "6        what is the forcaste for lincoln & hastings      weather_intent\n",
       "7                          current weather in dallas      weather_intent\n",
       "8                            what is the time system  information_intent\n",
       "9                     what is the strongest material  information_intent"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/marco_train_v3.csv\")\n",
    "print(len(df))\n",
    "print(df['target'].value_counts())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb24484d-7380-49b6-b67f-5b645e46cf49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    174823.000000\n",
      "mean         29.788781\n",
      "std          10.810686\n",
      "min           6.000000\n",
      "10%          18.000000\n",
      "20%          21.000000\n",
      "25%          22.000000\n",
      "30%          24.000000\n",
      "40%          26.000000\n",
      "50%          29.000000\n",
      "60%          31.000000\n",
      "70%          34.000000\n",
      "75%          35.000000\n",
      "80%          38.000000\n",
      "90%          43.000000\n",
      "95%          49.000000\n",
      "98%          55.000000\n",
      "99%          61.000000\n",
      "99.5%        69.000000\n",
      "99.8%        83.000000\n",
      "99.9%        95.000000\n",
      "max         193.000000\n",
      "Name: sequence, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df['sequence'].apply(len).describe(percentiles=[.1, .2, .25, .3, .4, .5, .6, .7, .75, .8, .9, .95, .98, .99, .995, .998, .999]))\n",
    "df['sequence'].apply(len).hist(bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "082fb996-037b-4353-8739-7047d6142f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>primetime emmy award for outstanding lead actress in a miniseries or a movie</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>Top-rated art restoration in Indianapolis, Indianasan antonio art restoration</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>what was one of the problems faced by the united states after world war ii?</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>how many people watched the world cup final compared to superbowl 2014</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>what is the difference between social economic and socio-economic</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174054</th>\n",
       "      <td>what is the appropriate techniques to stimulate a baby to breathe</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174114</th>\n",
       "      <td>Top-rated custom painting in Fort Wayne, INindianapolis custom painting</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174628</th>\n",
       "      <td>Top-rated fence installation in Virginia Beach, VAcleveland fence installation</td>\n",
       "      <td>yelp_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174721</th>\n",
       "      <td>a function of cholesterol that does not harm health is its role _____</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174766</th>\n",
       "      <td>which of the cognitive skills in critical thinking has to do with your ability to judge critique</td>\n",
       "      <td>information_intent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1290 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                sequence  \\\n",
       "223                         primetime emmy award for outstanding lead actress in a miniseries or a movie   \n",
       "462                        Top-rated art restoration in Indianapolis, Indianasan antonio art restoration   \n",
       "597                          what was one of the problems faced by the united states after world war ii?   \n",
       "626                               how many people watched the world cup final compared to superbowl 2014   \n",
       "686                                    what is the difference between social economic and socio-economic   \n",
       "...                                                                                                  ...   \n",
       "174054                                 what is the appropriate techniques to stimulate a baby to breathe   \n",
       "174114                           Top-rated custom painting in Fort Wayne, INindianapolis custom painting   \n",
       "174628                    Top-rated fence installation in Virginia Beach, VAcleveland fence installation   \n",
       "174721                             a function of cholesterol that does not harm health is its role _____   \n",
       "174766  which of the cognitive skills in critical thinking has to do with your ability to judge critique   \n",
       "\n",
       "                    target  \n",
       "223     information_intent  \n",
       "462            yelp_intent  \n",
       "597     information_intent  \n",
       "626     information_intent  \n",
       "686     information_intent  \n",
       "...                    ...  \n",
       "174054  information_intent  \n",
       "174114         yelp_intent  \n",
       "174628         yelp_intent  \n",
       "174721  information_intent  \n",
       "174766  information_intent  \n",
       "\n",
       "[1290 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.loc[df['sequence'].apply(lambda text: len(text) > 64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e0d5f3-b927-477e-ac5b-5e05fce4835b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  sequence  token_length\n",
      "0                  what's the ingredients for simple syrup            10\n",
      "1                     what is the basic unit of mass in si            11\n",
      "2                              Fresno, CA packing supplies             7\n",
      "3             does medicare pay for annual wellness visits             9\n",
      "4       Which is better: Samsung Galaxy or Samsung Galaxy?            12\n",
      "...                                                    ...           ...\n",
      "174818                              what is literary genre             6\n",
      "174819                                definition of avowed             6\n",
      "174820                           hair salons san francisco             7\n",
      "174821                          definition of disseminated             7\n",
      "174822                                       define psycho             4\n",
      "\n",
      "[174823 rows x 2 columns]\n",
      "Max token length: 55\n",
      "Average token length: 8.264044204709906\n",
      "90th percentile token length: 11.0\n",
      "95th percentile token length: 13.0\n",
      "98th percentile token length: 14.0\n",
      "99th percentile token length: 15.0\n",
      "99.5th percentile token length: 17.0\n",
      "99.9th percentile token length: 21.0\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "token_lengths = []\n",
    "for sequence in df['sequence'].values:\n",
    "    tokens = tokenizer(sequence, truncation=False)['input_ids']  # Get tokenized input IDs\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "temp_df = pd.DataFrame({'sequence': df['sequence'].values, 'token_length': token_lengths})\n",
    "\n",
    "# Display token lengths\n",
    "print(temp_df)\n",
    "\n",
    "# Optional: Analyze token lengths for deciding the best max_length\n",
    "print(f\"Max token length: {temp_df['token_length'].max()}\")\n",
    "print(f\"Average token length: {temp_df['token_length'].mean()}\")\n",
    "print(f\"90th percentile token length: {temp_df['token_length'].quantile(0.9)}\")\n",
    "print(f\"95th percentile token length: {temp_df['token_length'].quantile(0.95)}\")\n",
    "print(f\"98th percentile token length: {temp_df['token_length'].quantile(0.98)}\")\n",
    "print(f\"99th percentile token length: {temp_df['token_length'].quantile(0.99)}\")\n",
    "print(f\"99.5th percentile token length: {temp_df['token_length'].quantile(0.995)}\")\n",
    "print(f\"99.9th percentile token length: {temp_df['token_length'].quantile(0.999)}\")\n",
    "\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e6c934-fdc6-4d28-9c87-69eec74e5beb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    45874\n",
      "1    38241\n",
      "5    13195\n",
      "2     3498\n",
      "6     2159\n",
      "4     1717\n",
      "3     1320\n",
      "7        8\n",
      "Name: count, dtype: int64\n",
      "Size of sampled_df = 106012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_222172/2457747801.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('target', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>where cedar point is located</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>causes of heart fibrillation</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is chkdsk /f</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>define convocation ceremonies</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what kind of reaction occurs when you mix aqueous solutions of barium sulfide and sulfuric acid?</td>\n",
       "      <td>information_intent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           sequence  \\\n",
       "0                                                                      where cedar point is located   \n",
       "1                                                                      causes of heart fibrillation   \n",
       "2                                                                                 what is chkdsk /f   \n",
       "3                                                                     define convocation ceremonies   \n",
       "4  what kind of reaction occurs when you mix aqueous solutions of barium sulfide and sulfuric acid?   \n",
       "\n",
       "               target  label  \n",
       "0  information_intent      0  \n",
       "1  information_intent      0  \n",
       "2  information_intent      0  \n",
       "3  information_intent      0  \n",
       "4  information_intent      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only a sample from the actual data\n",
    "\n",
    "sampling_percentages = {\n",
    "    'information_intent': 0.4,   # 40% sampling for information_intent\n",
    "    'yelp_intent': 1.0,          # 100% sampling for yelp_intent\n",
    "    'weather_intent': 1.0,       # 100% sampling for weather_intent\n",
    "    'navigation_intent': 1.0,    # 100% sampling for navigation_intent\n",
    "    'purchase_intent': 1.0,      # 100% sampling for purchase_intent\n",
    "    'translation_intent': 1.0,   # 100% sampling for translation_intent\n",
    "    'travel_intent': 1.0,        # 100% sampling for travel_intent\n",
    "    'unknown': 1.0               # 100% sampling for unknown\n",
    "}\n",
    "\n",
    "# Sample from each target group based on the defined percentages\n",
    "sampled_df = df.groupby('target', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=sampling_percentages.get(x.name, 1.0))\n",
    ").reset_index(drop=True)\n",
    "\n",
    "sampled_df['label'] = sampled_df['target'].map(label2id)\n",
    "# sampled_df = sampled_df.rename(columns={'target': 'label'})\n",
    "\n",
    "print(sampled_df['label'].value_counts())\n",
    "print(f\"Size of sampled_df = {len(sampled_df)}\")\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dadb9a2-6184-4e50-bf21-8063e31edbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sequence', 'target', 'label'],\n",
      "        num_rows: 100711\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sequence', 'target', 'label'],\n",
      "        num_rows: 5301\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Split the DataFrame into train and validation sets\n",
    "train_df, val_df = train_test_split(sampled_df, test_size=0.05, random_state=42, stratify=sampled_df['label'])\n",
    "\n",
    "# Step 2: Convert Pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "# Step 3: Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "# Step 4: Verify the structure of DatasetDict\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0613b1e7-e4ec-4b18-91a2-2648c42a91b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    43580\n",
       "1    36329\n",
       "5    12535\n",
       "2     3323\n",
       "6     2051\n",
       "4     1631\n",
       "3     1254\n",
       "7        8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d476d6-4aa2-4180-889b-49f8e5251219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    2294\n",
       "1    1912\n",
       "5     660\n",
       "2     175\n",
       "6     108\n",
       "4      86\n",
       "3      66\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae60f7-3f28-4d95-bee7-28d05b68566f",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17cb2d99-4f2f-4caa-8f1f-747fbc9c9082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"sequence\"]\n",
    "\n",
    "    # tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"right\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,  # Pad the sequences to the longest in the batch\n",
    "        max_length=64\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d52d8863-7705-40d2-a6b4-26b1865f5c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100711/100711 [00:04<00:00, 24490.67 examples/s]\n",
      "Map: 100%|██████████| 5301/5301 [00:00<00:00, 26878.37 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'target', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 100711\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'target', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5301\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
    "# tokenized_dataset = tokenized_dataset.map(fix_labels)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56166c5b-958c-4921-961a-7e42c1ef37d7",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d481d862-dfe6-4635-9d0c-7c827a2155f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f08d8464-0714-4021-a266-9ceb64f36f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "floor repair cost -> purchase_intent\n",
      "denture fix -> purchase_intent\n",
      "who is the us president -> purchase_intent\n",
      "italian food -> purchase_intent\n",
      "sandwiches in seattle -> purchase_intent\n",
      "seattle weather -> purchase_intent\n",
      "weather seattle -> purchase_intent\n",
      "boston wether -> purchase_intent\n",
      "Boston wether -> purchase_intent\n",
      "weather boston -> purchase_intent\n",
      "weather Boston -> purchase_intent\n",
      "Weather Boston -> purchase_intent\n",
      "weathr boston -> purchase_intent\n",
      "seattle weathr -> purchase_intent\n",
      "apple macbook price -> purchase_intent\n",
      "sf sushi -> purchase_intent\n",
      "sf ramen -> purchase_intent\n",
      "seattle sushi -> purchase_intent\n",
      "seattle ramen -> purchase_intent\n",
      "sushi sf -> purchase_intent\n",
      "ramen sf -> purchase_intent\n"
     ]
    }
   ],
   "source": [
    "### Evaluate untrained model\n",
    "\n",
    "text_list = [\n",
    "    'floor repair cost',\n",
    "    'denture fix',\n",
    "    'who is the us president',\n",
    "    'italian food',\n",
    "    'sandwiches in seattle',\n",
    "    'seattle weather',\n",
    "    'weather seattle',\n",
    "    'boston wether',\n",
    "    'Boston wether',\n",
    "    'weather boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weathr boston',\n",
    "    'seattle weathr',\n",
    "    'apple macbook price',\n",
    "    'sf sushi',\n",
    "    'sf ramen',\n",
    "    'seattle sushi',\n",
    "    'seattle ramen',\n",
    "    'sushi sf',\n",
    "    'ramen sf',\n",
    "]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "predictions = []\n",
    "logits_list = []\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    logits = model(inputs).logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    predictions.append(prediction)\n",
    "    print(text + \" -> \" + id2label[prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfb5ad-391d-42da-bcf5-d62e39642225",
   "metadata": {},
   "source": [
    "#### Model finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cece8cd8-b017-4f80-9d9a-713041270fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 28,680 || all params: 24,614,672 || trainable%: 0.1165\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",\n",
    "                         r=4, # intrinsic rank of trainable weight matrix\n",
    "                         lora_alpha=32, # similar to learning_rate\n",
    "                         lora_dropout=0.01, # probability of dropout nodes\n",
    "                         target_modules=['attention.self.query']) # LoRA is applied to query layer\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93c9ac1c-cd57-4173-b208-78831d9c4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccdcde-6b2c-499d-a185-d04dbd1b6ba7",
   "metadata": {},
   "source": [
    "#### Define hyper parameters and training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84297802-ae8a-4fef-882a-7d7145de72bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-5\n",
    "batch_size = 16\n",
    "num_epochs = 15\n",
    "\n",
    "# training args\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=model_checkpoint + \"-lora-intent-classification-v3\",\n",
    "    output_dir=\"mobilebert-uncased\" + \"-lora-intent-classification-v3\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615cdb83-edc7-409a-a929-6c370c8aa9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, # Hyperparamaters\n",
    "    train_dataset=tokenized_dataset[\"train\"], # training data\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # validation data\n",
    "    tokenizer=tokenizer, # tokenizer\n",
    "    data_collator=data_collator, # dynamic sequence padding\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9fde457-1a0d-4552-b6fa-1c0642349972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94425' max='94425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94425/94425 2:34:14, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>55.725400</td>\n",
       "      <td>10.873102</td>\n",
       "      <td>0.180721</td>\n",
       "      <td>0.100661</td>\n",
       "      <td>0.180721</td>\n",
       "      <td>0.125411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.590700</td>\n",
       "      <td>1.494124</td>\n",
       "      <td>0.776835</td>\n",
       "      <td>0.751875</td>\n",
       "      <td>0.776835</td>\n",
       "      <td>0.753953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12.388700</td>\n",
       "      <td>1.105038</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.839658</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.825295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11.157800</td>\n",
       "      <td>4.252795</td>\n",
       "      <td>0.853613</td>\n",
       "      <td>0.853963</td>\n",
       "      <td>0.853613</td>\n",
       "      <td>0.845536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>19.261300</td>\n",
       "      <td>1.178998</td>\n",
       "      <td>0.890775</td>\n",
       "      <td>0.887651</td>\n",
       "      <td>0.890775</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.340100</td>\n",
       "      <td>0.752808</td>\n",
       "      <td>0.900208</td>\n",
       "      <td>0.899750</td>\n",
       "      <td>0.900208</td>\n",
       "      <td>0.896352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>9.866600</td>\n",
       "      <td>0.992897</td>\n",
       "      <td>0.900396</td>\n",
       "      <td>0.904801</td>\n",
       "      <td>0.900396</td>\n",
       "      <td>0.900616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>16.979900</td>\n",
       "      <td>1.051334</td>\n",
       "      <td>0.910394</td>\n",
       "      <td>0.909159</td>\n",
       "      <td>0.910394</td>\n",
       "      <td>0.907964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9.227400</td>\n",
       "      <td>1.334837</td>\n",
       "      <td>0.910583</td>\n",
       "      <td>0.909220</td>\n",
       "      <td>0.910583</td>\n",
       "      <td>0.907760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.111000</td>\n",
       "      <td>0.994609</td>\n",
       "      <td>0.914356</td>\n",
       "      <td>0.913062</td>\n",
       "      <td>0.914356</td>\n",
       "      <td>0.912236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>10.512100</td>\n",
       "      <td>0.749656</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>0.914336</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>0.912785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.909000</td>\n",
       "      <td>0.325421</td>\n",
       "      <td>0.926995</td>\n",
       "      <td>0.925208</td>\n",
       "      <td>0.926995</td>\n",
       "      <td>0.924737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.651700</td>\n",
       "      <td>0.425194</td>\n",
       "      <td>0.917185</td>\n",
       "      <td>0.916658</td>\n",
       "      <td>0.917185</td>\n",
       "      <td>0.915772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.351600</td>\n",
       "      <td>0.323814</td>\n",
       "      <td>0.917940</td>\n",
       "      <td>0.922459</td>\n",
       "      <td>0.917940</td>\n",
       "      <td>0.919486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.530600</td>\n",
       "      <td>0.280181</td>\n",
       "      <td>0.931522</td>\n",
       "      <td>0.929942</td>\n",
       "      <td>0.931522</td>\n",
       "      <td>0.929783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=94425, training_loss=3915.3147779409474, metrics={'train_runtime': 9255.0259, 'train_samples_per_second': 163.226, 'train_steps_per_second': 10.203, 'total_flos': 7663701478254624.0, 'train_loss': 3915.3147779409474, 'epoch': 15.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1da3c5f1-ef7f-4ca1-9792-4879b621f267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d874c9-e652-4c17-8a54-11d94710debb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floor repair cost -> yelp_intent\n",
      "denture fix -> yelp_intent\n",
      "who is the us president -> information_intent\n",
      "italian food -> yelp_intent\n",
      "sandwiches in seattle -> yelp_intent\n",
      "seattle weather -> weather_intent\n",
      "weather seattle -> weather_intent\n",
      "boston wether -> weather_intent\n",
      "Boston wether -> weather_intent\n",
      "weather boston -> weather_intent\n",
      "weather Boston -> weather_intent\n",
      "Weather Boston -> weather_intent\n",
      "weathr boston -> yelp_intent\n",
      "seattle weathr -> weather_intent\n",
      "apple macbook price -> yelp_intent\n",
      "sf sushi -> yelp_intent\n",
      "sf ramen -> yelp_intent\n",
      "seattle sushi -> yelp_intent\n",
      "seattle ramen -> yelp_intent\n",
      "sushi sf -> yelp_intent\n",
      "ramen sf -> yelp_intent\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in text_list:\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "        logits = trainer.model(inputs).logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        print(text + \" -> \" + id2label[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66aae059-b207-43d5-8a78-a76901550c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.4M\n",
      "-rw-r--r-- 1 jupyter jupyter 5.0K Oct 20 19:21 README.md\n",
      "-rw-r--r-- 1 jupyter jupyter  679 Oct 20 19:21 adapter_config.json\n",
      "-rw-r--r-- 1 jupyter jupyter 119K Oct 20 19:21 adapter_model.safetensors\n",
      "-rw-r--r-- 1 jupyter jupyter 266K Oct 20 19:21 optimizer.pt\n",
      "-rw-r--r-- 1 jupyter jupyter  14K Oct 20 19:21 rng_state.pth\n",
      "-rw-r--r-- 1 jupyter jupyter 1.1K Oct 20 19:21 scheduler.pt\n",
      "-rw-r--r-- 1 jupyter jupyter  125 Oct 20 19:21 special_tokens_map.json\n",
      "-rw-r--r-- 1 jupyter jupyter 695K Oct 20 19:21 tokenizer.json\n",
      "-rw-r--r-- 1 jupyter jupyter 1.3K Oct 20 19:21 tokenizer_config.json\n",
      "-rw-r--r-- 1 jupyter jupyter  38K Oct 20 19:21 trainer_state.json\n",
      "-rw-r--r-- 1 jupyter jupyter 5.2K Oct 20 19:21 training_args.bin\n",
      "-rw-r--r-- 1 jupyter jupyter 227K Oct 20 19:21 vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -lh mobilebert-uncased-lora-intent-classification-v3/checkpoint-94425"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08157f-ff0c-46a3-aa0d-f04493a71d36",
   "metadata": {},
   "source": [
    "#### Load the LoRA model from checkpoint after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "041cee58-787b-430c-a9c4-c4aadcf87a58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6269,  3.1033,  0.4597, -2.6642,  1.1138, -4.8732, -4.0590, -8.7972]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "1 yelp_intent\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: 'information_intent',\n",
    "            1: 'yelp_intent',\n",
    "            2: 'navigation_intent',\n",
    "            3: 'travel_intent',\n",
    "            4: 'purchase_intent',\n",
    "            5: 'weather_intent',\n",
    "            6: 'translation_intent',\n",
    "            7: 'unknown'}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "\n",
    "\n",
    "output_dir = \"mobilebert-uncased-lora-intent-classification-v3/checkpoint-94425\"\n",
    "\n",
    "# Load the tokenizer (from the output directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Load the base model from the original checkpoint (base pre-trained model)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Load the LoRA configuration and model\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Step 3: Save the combined model to a directory\n",
    "save_directory = \"tmp/mobilebert_lora_combined_model/\"\n",
    "lora_model.save_pretrained(save_directory)  # Save base model + LoRA weights\n",
    "\n",
    "# Now the `lora_model` contains both the base model and the LoRA weights.\n",
    "lora_model.eval()\n",
    "\n",
    "# Example inference\n",
    "inputs = tokenizer([\"looking for home cleaning \"], return_tensors=\"pt\")\n",
    "outputs = lora_model(**inputs)\n",
    "logits = outputs.logits\n",
    "print(logits)\n",
    "\n",
    "\n",
    "prediction = torch.argmax(logits, dim=1).item()\n",
    "print(prediction, id2label[prediction])\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "rounded_probabilities = torch.round(probabilities)\n",
    "print(rounded_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5f259a5-9032-4614-b0fc-b6c685ea250e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tmp/mobilebert_lora_combined_model/tokenizer_config.json',\n",
       " 'tmp/mobilebert_lora_combined_model/special_tokens_map.json',\n",
       " 'tmp/mobilebert_lora_combined_model/vocab.txt',\n",
       " 'tmp/mobilebert_lora_combined_model/added_tokens.json',\n",
       " 'tmp/mobilebert_lora_combined_model/tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Step 1: Load the base model (DistilBERT)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Step 2: Load the LoRA adapter weights\n",
    "output_dir = \"mobilebert-uncased-lora-intent-classification-v3/checkpoint-94425\"\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "lora_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Step 3: Merge LoRA weights into the base model\n",
    "# After this, the model will have both base and LoRA weights applied\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Step 4: Save the full model (base model + LoRA weights)\n",
    "save_directory = \"tmp/mobilebert_lora_combined_model/\"\n",
    "merged_model.save_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)  # Load the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)  # Save the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7eb33ea5-05c4-4417-8c86-05782471d680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75982821-faaf-45ba-9936-26f1c275f78f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 98.5M/98.5M [00:02<00:00, 39.1MB/s]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier/commit/81bf813a9612dd913d03b7665ce5524c859924a2', commit_message='Upload tokenizer', commit_description='', oid='81bf813a9612dd913d03b7665ce5524c859924a2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_dir = \"tmp/mobilebert_lora_combined_model\"\n",
    "merged_repo_id = \"Mozilla/mobilebert-uncased-finetuned-LoRA-intent-classifier\"  \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1bcba70-77f1-493e-8c35-e135ec2f9893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ce05e-44be-4fc1-b8bb-ada9f1415386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
