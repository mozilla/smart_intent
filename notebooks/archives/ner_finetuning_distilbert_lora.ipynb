{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bbbb75-8c65-440e-a059-c63c2fa91996",
   "metadata": {},
   "source": [
    "purpose of this notebook is to finetune the \"distilbert/distilbert-base-uncased\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7436463-26a4-4ebb-abd1-771ee134220b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6740344d-7d09-457a-bb68-a64f2b532103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'mps'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6093b56-0180-4b67-8b04-b562979979ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full_dataset = Dataset.from_parquet(\"data/combined_ner_examples.parquet\")\n",
    "full_dataset = Dataset.from_parquet(\"data/combined_ner_examples_v2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f06a123-bdd7-4655-ae16-92bdc655924f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 24041\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5b0e6e-e169-4e32-aeae-00c58949ff32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23041"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set_size = 1000\n",
    "val_start = len(full_dataset) - val_set_size\n",
    "val_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3d3524-3a60-4ce2-863b-08a6dadd2fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_dataset = full_dataset.select(range(val_start))  # Select training rows\n",
    "val_dataset = full_dataset.select(range(val_start, len(full_dataset)))  # Select last 1000 rows for validation\n",
    "\n",
    "# Combine them into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ffc3cd-886d-4f7f-a8c8-8f5413628646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 23041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340175ec-d224-4c3b-aaa5-16037c61fff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'].to_pandas()['tokens'].apply(len).hist(bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2046667-d9ef-44ab-a444-dc95752478aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f20d0f6-fa8e-47d7-a60b-2e673e25685a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 3667.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for distilbert-based NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Function to tokenize the input and align labels with tokens\n",
    "def tokenize_and_align_labels(example):\n",
    "    # Tokenize 'tokens' while keeping track of word boundaries\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example['tokens'], \n",
    "        is_split_into_words=True, \n",
    "        truncation=True, \n",
    "        padding='max_length',\n",
    "        max_length=64,\n",
    "    )\n",
    "    \n",
    "    # Get the word_ids (mapping from tokens to original words)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    aligned_labels = []\n",
    "\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)  # Special tokens ([CLS], [SEP], etc.)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(example['ner_tags'][word_idx])  # Assign the label to the first token of each word\n",
    "        else:\n",
    "            aligned_labels.append(-100)  # Subword tokens get label -100\n",
    "\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the function to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106935d0-9093-4ff9-8e0f-afe4aa2d792c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 23041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "331d6dca-55d0-473e-94ca-90a152a9e9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 23041,\n",
       " 'tokens': ['John', 'visited', 'Albuquerque', 'last', 'summer.'],\n",
       " 'ner_tags': [0, 0, 5, 0, 0],\n",
       " 'input_ids': [101,\n",
       "  2198,\n",
       "  4716,\n",
       "  19334,\n",
       "  2197,\n",
       "  2621,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1221fd4e-3f16-4ed4-9d9f-c359604545ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_predictions_and_labels(predictions, references):\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    cmp_count = 0\n",
    "\n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        # Only keep labels that are not -100\n",
    "        true_labels_example = [label for label in reference if label != -100]\n",
    "        \n",
    "        # Align predictions: Remove predictions for which the corresponding reference label is -100\n",
    "        true_predictions_example = [pred for pred, ref in zip(prediction, reference) if ref != -100]\n",
    "\n",
    "        # Ensure the length of predictions and labels matches\n",
    "        if len(true_predictions_example) == len(true_labels_example):\n",
    "            true_labels.append(true_labels_example)\n",
    "            true_predictions.append(true_predictions_example)\n",
    "            cmp_count += 1\n",
    "        else:\n",
    "            # Log or handle the error (example-level mismatch)\n",
    "            # print(f\"Skipping example due to mismatch: predictions ({len(true_predictions_example)}), labels ({len(true_labels_example)})\")\n",
    "            continue  # Skip this example\n",
    "\n",
    "    # Flatten the lists (convert from list of lists to a single list)\n",
    "    true_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
    "    true_labels = [label for sublist in true_labels for label in sublist]\n",
    "    print(f\"cmp_count = {cmp_count} out of {len(predictions)}\")\n",
    "\n",
    "    return true_predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f62dee2-077d-451f-af48-60eaf50e5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Post-process the predictions and labels to remove -100 values\n",
    "    true_predictions, true_labels = postprocess_predictions_and_labels(predictions, labels)\n",
    "\n",
    "    # Combine metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f7acd16-763a-4055-b492-3007b5057da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the NER label mappings\n",
    "id2label = {\n",
    "    0: \"O\",         # Outside any entity\n",
    "    1: \"B-PER\",     # Beginning of a person entity\n",
    "    2: \"I-PER\",     # Inside a person entity\n",
    "    3: \"B-ORG\",     # Beginning of an organization entity\n",
    "    4: \"I-ORG\",     # Inside an organization entity\n",
    "    5: \"B-LOC\",     # Beginning of a location entity\n",
    "    6: \"I-LOC\",     # Inside a location entity\n",
    "    7: \"B-MISC\",    # Beginning of a miscellaneous entity\n",
    "    8: \"I-MISC\"     # Inside a miscellaneous entity\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", \n",
    "                                                        num_labels=9, \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,  # Task type is token classification (NER)\n",
    "    r=8,  # Low-rank dimension (you can experiment with this)\n",
    "    lora_alpha=32,  # Scaling factor for LoRA\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    target_modules=['q_lin']  # LoRA is applied to query layer\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "343c2425-f8a2-4d13-a1cc-22ed6a4717a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c185799-1fd7-4319-a2a5-89ba1ff52df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17292' max='17292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17292/17292 17:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.879700</td>\n",
       "      <td>0.516633</td>\n",
       "      <td>0.028251</td>\n",
       "      <td>0.378969</td>\n",
       "      <td>0.028251</td>\n",
       "      <td>0.049853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.429300</td>\n",
       "      <td>0.336937</td>\n",
       "      <td>0.034088</td>\n",
       "      <td>0.136995</td>\n",
       "      <td>0.034088</td>\n",
       "      <td>0.052876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.270327</td>\n",
       "      <td>0.037357</td>\n",
       "      <td>0.117048</td>\n",
       "      <td>0.037357</td>\n",
       "      <td>0.053422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.234538</td>\n",
       "      <td>0.037590</td>\n",
       "      <td>0.119460</td>\n",
       "      <td>0.037590</td>\n",
       "      <td>0.053924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>0.206721</td>\n",
       "      <td>0.038291</td>\n",
       "      <td>0.127506</td>\n",
       "      <td>0.038291</td>\n",
       "      <td>0.055695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.182278</td>\n",
       "      <td>0.039225</td>\n",
       "      <td>0.144757</td>\n",
       "      <td>0.039225</td>\n",
       "      <td>0.058398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.162497</td>\n",
       "      <td>0.039225</td>\n",
       "      <td>0.147928</td>\n",
       "      <td>0.039225</td>\n",
       "      <td>0.058713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.145596</td>\n",
       "      <td>0.039458</td>\n",
       "      <td>0.157392</td>\n",
       "      <td>0.039458</td>\n",
       "      <td>0.059710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.134719</td>\n",
       "      <td>0.040392</td>\n",
       "      <td>0.161956</td>\n",
       "      <td>0.040392</td>\n",
       "      <td>0.061109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>0.127017</td>\n",
       "      <td>0.040626</td>\n",
       "      <td>0.912195</td>\n",
       "      <td>0.040626</td>\n",
       "      <td>0.061750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.123017</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.917123</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.062367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.121716</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.917193</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.062403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmp_count = 885 out of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/new_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17292, training_loss=0.346677775077458, metrics={'train_runtime': 1022.3818, 'train_samples_per_second': 270.439, 'train_steps_per_second': 16.913, 'total_flos': 4524701386097664.0, 'train_loss': 0.346677775077458, 'epoch': 12.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",           # Output directory\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate at the end of every epoch\n",
    "    learning_rate=1e-5,               # Learning rate\n",
    "    per_device_train_batch_size=16,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
    "    num_train_epochs=12,               # Number of training epochs\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    logging_dir='./logs',             # Directory for logging\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,                  # LoRA-wrapped model\n",
    "    args=training_args,                # Training arguments\n",
    "    train_dataset=tokenized_dataset['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],  # Validation dataset (if available)\n",
    "    tokenizer=tokenizer,               # Tokenizer\n",
    "    compute_metrics=compute_metrics,  # model perfomance evaluation metric\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82880e92-4d83-442a-9a1c-3a2386b1c942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] O\n",
      "new B-LOC\n",
      "york I-LOC\n",
      "[SEP] O\n",
      "Input: New York\n",
      "Predicted entities: new york\n",
      "\n",
      "[CLS] O\n",
      "los B-LOC\n",
      "angeles I-LOC\n",
      "[SEP] O\n",
      "Input: Los Angeles\n",
      "Predicted entities: los angeles\n",
      "\n",
      "[CLS] O\n",
      "chicago B-LOC\n",
      "[SEP] O\n",
      "Input: Chicago\n",
      "Predicted entities: chicago\n",
      "\n",
      "[CLS] O\n",
      "philadelphia B-LOC\n",
      "[SEP] O\n",
      "Input: Philadelphia\n",
      "Predicted entities: philadelphia\n",
      "\n",
      "[CLS] O\n",
      "dallas B-LOC\n",
      "[SEP] O\n",
      "Input: Dallas\n",
      "Predicted entities: dallas\n",
      "\n",
      "[CLS] O\n",
      "fort B-LOC\n",
      "worth I-LOC\n",
      "[SEP] O\n",
      "Input: Fort Worth\n",
      "Predicted entities: fort worth\n",
      "\n",
      "[CLS] O\n",
      "houston B-LOC\n",
      "[SEP] O\n",
      "Input: Houston\n",
      "Predicted entities: houston\n",
      "\n",
      "[CLS] O\n",
      "atlanta B-LOC\n",
      "[SEP] O\n",
      "Input: Atlanta\n",
      "Predicted entities: atlanta\n",
      "\n",
      "[CLS] O\n",
      "boston B-LOC\n",
      "[SEP] O\n",
      "Input: Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "manchester B-LOC\n",
      "[SEP] O\n",
      "Input: Manchester\n",
      "Predicted entities: manchester\n",
      "\n",
      "[CLS] O\n",
      "washington B-LOC\n",
      ", O\n",
      "d B-LOC\n",
      ". O\n",
      "c I-LOC\n",
      ". O\n",
      "[SEP] O\n",
      "Input: Washington, D.C.\n",
      "Predicted entities: washington d c\n",
      "\n",
      "[CLS] O\n",
      "ha B-LOC\n",
      "##gers B-LOC\n",
      "##town I-LOC\n",
      "[SEP] O\n",
      "Input: Hagerstown\n",
      "Predicted entities: hagerstown\n",
      "\n",
      "[CLS] O\n",
      "san B-LOC\n",
      "francisco I-LOC\n",
      "[SEP] O\n",
      "Input: San Francisco\n",
      "Predicted entities: san francisco\n",
      "\n",
      "[CLS] O\n",
      "oakland B-LOC\n",
      "[SEP] O\n",
      "Input: Oakland\n",
      "Predicted entities: oakland\n",
      "\n",
      "[CLS] O\n",
      "san B-LOC\n",
      "jose I-LOC\n",
      "[SEP] O\n",
      "Input: San Jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "san B-LOC\n",
      "jose I-LOC\n",
      "[SEP] O\n",
      "Input: weather in san jose\n",
      "Predicted entities: san jose\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "boston B-LOC\n",
      "[SEP] O\n",
      "Input: weather in Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "in O\n",
      "boston B-LOC\n",
      "[SEP] O\n",
      "Input: Weather in Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "boston B-LOC\n",
      "[SEP] O\n",
      "Input: weather Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "boston B-LOC\n",
      "[SEP] O\n",
      "Input: Weather Boston\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: Weather\n",
      "Predicted entities: \n",
      "\n",
      "[CLS] O\n",
      "boston B-LOC\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: Boston weather\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "boston B-LOC\n",
      "weather O\n",
      "[SEP] O\n",
      "Input: Boston Weather\n",
      "Predicted entities: boston\n",
      "\n",
      "[CLS] O\n",
      "i O\n",
      "love O\n",
      "pizza B-LOC\n",
      "##hu O\n",
      "##t O\n",
      "[SEP] O\n",
      "Input: I love Pizzahut\n",
      "Predicted entities: pizzahut\n",
      "\n",
      "[CLS] O\n",
      "i O\n",
      "like O\n",
      "starbucks O\n",
      "[SEP] O\n",
      "Input: I like Starbucks\n",
      "Predicted entities: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your text list\n",
    "text_list = [\n",
    "    'New York', 'Los Angeles', 'Chicago', 'Philadelphia', 'Dallas',\n",
    "    'Fort Worth', 'Houston', 'Atlanta', 'Boston', 'Manchester',\n",
    "    'Washington, D.C.', 'Hagerstown', 'San Francisco', 'Oakland',\n",
    "    'San Jose', \n",
    "    # 'san jose',\n",
    "    'weather in san jose',\n",
    "    'weather in Boston',\n",
    "    'Weather in Boston',\n",
    "    'weather Boston',\n",
    "    'Weather Boston',\n",
    "    'weather',\n",
    "    'Weather',\n",
    "    'Boston weather',\n",
    "    'Boston Weather',\n",
    "    'I love Pizzahut',\n",
    "    'I like Starbucks',\n",
    "]\n",
    "\n",
    "model = trainer.model\n",
    "\n",
    "# Function to make predictions and group entities\n",
    "def predict_ner(text_list):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    for text in text_list:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predictions (logits -> predicted labels)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "        \n",
    "        # Map the predictions to labels and tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu().numpy())\n",
    "        ner_labels = [model.config.id2label[pred] for pred in predictions]\n",
    "\n",
    "        # Group tokens back into entities\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        entities = []\n",
    "\n",
    "        for token, label in zip(tokens, ner_labels):\n",
    "            print(token, label)\n",
    "            # Ignore special tokens like [CLS], [SEP]\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            # Handle subword tokens (tokens starting with ##)\n",
    "            if token.startswith(\"##\"):\n",
    "                current_entity[-1] += token[2:]  # Append the subword without \"##\"\n",
    "            elif label.startswith(\"B-\") or (label.startswith(\"I-\") and label != current_label):\n",
    "                # New entity starts, append the old one\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_entity.append(token)\n",
    "                current_label = label\n",
    "            elif label.startswith(\"I-\") and label == current_label:\n",
    "                # Continue current entity\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                # Non-entity token or 'O'\n",
    "                if current_entity:\n",
    "                    entities.append(\" \".join(current_entity))\n",
    "                    current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "        # Append any remaining entity\n",
    "        if current_entity:\n",
    "            entities.append(\" \".join(current_entity))\n",
    "\n",
    "        # Clean up tokens (remove subword tokens and punctuation issues, etc.)\n",
    "        clean_entities = []\n",
    "        for entity in entities:\n",
    "            entity = entity.replace(\" ##\", \"\")\n",
    "            entity = entity.replace(\" .\", \".\")  # Handle punctuation\n",
    "            entity = entity.replace(\" ,\", \",\")\n",
    "            clean_entities.append(entity)\n",
    "\n",
    "        # Print the result for comparison\n",
    "        print(f\"Input: {text}\")\n",
    "        print(f\"Predicted entities: {' '.join(clean_entities)}\")\n",
    "        print()\n",
    "\n",
    "# Run predictions on the text list\n",
    "predict_ner(text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdf1c664-80e1-47a1-a33f-98e2dd509623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jupyter/new_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the base model (DistilBERT NER model)\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\",\n",
    "                                                             num_labels=9,\n",
    "                                                             id2label=id2label,\n",
    "                                                             label2id=label2id)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Load the LoRA-adapted model\n",
    "peft_config = PeftConfig.from_pretrained(\"results/checkpoint-17292\")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"results/checkpoint-17292\")\n",
    "\n",
    "# Merge the LoRA weights with the base model\n",
    "merged_model = lora_model.merge_and_unload()  # This merges LoRA into the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9d63ba4-2659-44b6-bf40-e33cc1516545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tmp/merged_distilbert_uncased_ner/tokenizer_config.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/special_tokens_map.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/vocab.txt',\n",
       " 'tmp/merged_distilbert_uncased_ner/added_tokens.json',\n",
       " 'tmp/merged_distilbert_uncased_ner/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the merged model and tokenizer\n",
    "save_dir = \"tmp/merged_distilbert_uncased_ner\"\n",
    "merged_model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efae28df-7596-4142-9aa8-9e8e5d291f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b73fbf99-9165-4afe-a34a-a7a112427371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c889aa6-4b3d-479b-8dc7-23abf7a3164e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 265M/265M [00:06<00:00, 39.7MB/s] \n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Mozilla/distilbert-uncased-NER-LoRA/commit/1b1c607b5d2d2489c034703bbc8dcc33610debb4', commit_message='Upload tokenizer', commit_description='', oid='1b1c607b5d2d2489c034703bbc8dcc33610debb4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Mozilla/distilbert-uncased-NER-LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='Mozilla/distilbert-uncased-NER-LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the merged model\n",
    "merged_model_dir = \"tmp/merged_distilbert_uncased_ner\"\n",
    "merged_repo_id = \"Mozilla/distilbert-uncased-NER-LoRA\" \n",
    "\n",
    "merged_model.push_to_hub(merged_repo_id)\n",
    "tokenizer.push_to_hub(merged_repo_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f08c0-2488-4efb-93b2-2d89a8ff6cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "my_env",
   "name": ".m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m124"
  },
  "kernelspec": {
   "display_name": "Python (my_env) (Local)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
